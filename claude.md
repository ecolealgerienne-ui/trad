# Crypto Trading Signal Prediction - Pipeline de Donn√©es IA

## Description du Projet

Mod√®le d'IA pour pr√©dire la direction de la tendance du Bitcoin (BTC) et autres cryptomonnaies en utilisant la **reconstruction de signal** bas√©e sur des filtres d'Octave.

**D√©fi Principal** : Les prix sont trop bruit√©s pour √™tre pr√©dits directement. Nous utilisons des filtres math√©matiques (filtfilt) pour cr√©er un signal "propre", mais ces filtres n√©cessitent de conna√Ætre le futur. L'IA doit apprendre √† reconstruire la pente de ce signal parfait en n'utilisant que les donn√©es pr√©sentes.

**Objectif** : Atteindre une pr√©cision (Accuracy) > 90% sur la pr√©diction de la pente du signal filtr√©.

---

## ‚ö†Ô∏è Configuration Importante

### GPU OBLIGATOIRE
**TOUJOURS privil√©gier l'utilisation du GPU pour ce projet.**

- Configurer TensorFlow/PyTorch pour utiliser le GPU
- V√©rifier la disponibilit√© du GPU avant l'entra√Ænement
- Optimiser les batch sizes en fonction de la m√©moire GPU disponible

### Environnement Conda
Conda est activ√© sur la machine. Utiliser l'environnement `base` ou cr√©er un environnement d√©di√©.

---

## Architecture du Projet

### Structure des Dossiers

```
trad/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                 # Lien vers ../data_trad (donn√©es 5min sources)
‚îÇ   ‚îî‚îÄ‚îÄ processed/           # Dataset 30min g√©n√©r√© (bougie fant√¥me)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ utils.py            # Fonctions communes PARTAG√âES (NO DUPLICATION)
‚îÇ   ‚îú‚îÄ‚îÄ filters.py          # Filtre d'Octave (scipy.signal.filtfilt)
‚îÇ   ‚îú‚îÄ‚îÄ indicators.py       # RSI, CCI, MACD, Bollinger
‚îÇ   ‚îú‚îÄ‚îÄ normalization.py    # Z-Score, Relative Open
‚îÇ   ‚îú‚îÄ‚îÄ labeling.py         # Calcul pente d√©cal√©e (t-1 vs t-2)
‚îÇ   ‚îî‚îÄ‚îÄ data_pipeline.py    # Pipeline principal (Bougie Fant√¥me 30m)
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ 01_data_validation.ipynb
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ claude.md               # Ce fichier
‚îî‚îÄ‚îÄ README.md
```

### Donn√©es Sources

**Localisation** : `../data_trad/`
- `BTCUSD_all_5m.csv` : Donn√©es Bitcoin 5 minutes
- `ETHUSD_all_5m.csv` : Donn√©es Ethereum 5 minutes

---

## Spec #1 : Pipeline de Donn√©es (Phase Actuelle)

### 1. Concept : Multi-Timeframe Intra-bar

Le mod√®le travaille sur des **bougies de 30 minutes**, mais "voit" ce qu'il se passe **toutes les 5 minutes** √† l'int√©rieur.

#### La "Bougie Fant√¥me" (Snapshot)

√Ä chaque intervalle de 5 minutes, g√©n√©rer un vecteur d'√©tat qui agr√®ge la bougie de 30m en cours :

```
t=0min  : Bougie 30m commence
t=5min  : [O, H, L, C] bas√© sur 1√®re bougie 5m
t=10min : [O, H, L, C] mis √† jour avec 2 premi√®res bougies 5m
t=15min : [O, H, L, C] mis √† jour avec 3 premi√®res bougies 5m
t=20min : [O, H, L, C] mis √† jour avec 4 premi√®res bougies 5m
t=25min : [O, H, L, C] mis √† jour avec 5 premi√®res bougies 5m
t=30min : Bougie 30m compl√®te (6 bougies 5m)
```

#### Normalisation Multi-Actifs

**JAMAIS utiliser de prix bruts** pour universalit√© (BTC, ETH, etc.)

1. **Z-Score Glissant** : `(price - mean) / std` sur fen√™tre mobile
2. **Relative Open** : Exprimer H, L, C en % de l'Open de la bougie 30m

### 2. La Cible de Pr√©diction (Labeling) ‚ö†Ô∏è CRITIQUE

#### Signal Cible (Y)
- Appliquer filtre d'Octave (`scipy.signal.filtfilt`) avec param√®tre de lissage `0.25`
- Sur l'indicateur **RSI** (ou Close)

#### Pente
- Ne PAS pr√©dire la valeur, mais le **signe de la pente**
- Pente positive ‚Üí Label = 1
- Pente n√©gative ‚Üí Label = 0

#### D√©calage (Offset)
**Le mod√®le √† l'instant t doit pr√©dire si la pente entre t-2 et t-1 est positive.**

```python
# Pseudo-code
signal_filtered = filtfilt(signal)  # Utilise pass√© + futur
slope = signal_filtered[t-1] - signal_filtered[t-2]
label[t] = 1 if slope > 0 else 0
```

Note : On utilise les donn√©es de t pour pr√©dire le point juste avant (stabilisation).

### 3. Features Engineering (Inputs)

Indicateurs √† calculer et **stacker** (empiler) :
- **RSI** (Relative Strength Index)
- **CCI** (Commodity Channel Index)
- **MACD** (Moving Average Convergence Divergence)
- **Bandes de Bollinger**

Chaque indicateur sur **plusieurs fen√™tres temporelles** (multi-timeframe).

---

## R√®gles de D√©veloppement

### 1. Partage de Code - Z√âRO DUPLICATION
- **TOUJOURS v√©rifier si une fonction existe d√©j√†** avant de la r√©√©crire
- Mettre les fonctions communes dans `utils.py`
- R√©utiliser entre tous les scripts

### 2. Pr√©vention de Data Leakage
- **JAMAIS utiliser de donn√©es futures** dans les features
- Le filtre d'Octave utilise le futur UNIQUEMENT pour le label (cible)
- Valider qu'aucune information de t+1 ne fuite dans les inputs de t

### 3. Format de Sortie
- Dataset final : **CSV/DataFrame**
- Colonnes : `[timestamp, feat1, feat2, ..., featN, label]`
- Label : `{0, 1}` (pente n√©gative ou positive)

---

## üÜï Mise √† Jour CRITIQUE: Filtres Adaptatifs Zero-Lag (2026-01-01)

### Objectif: Path vers 90%+ Accuracy

**Probl√®me identifi√©:** Les filtres statiques ont un lag fixe qui nuit √† la pr√©cision.

**Solution:** Filtres adaptatifs qui s'ajustent dynamiquement au march√©.

### Architecture Mise √† Jour

```
FEATURES (X) - STRICTEMENT CAUSALES:
‚îú‚îÄ Ghost Candles (O, H, L, C)
‚îú‚îÄ Features Avanc√©es (velocity, amplitude, log returns, Z-Score)
‚îú‚îÄ Indicateurs Classiques (RSI, CCI, MACD, BB)
‚îî‚îÄ üÜï FILTRES ADAPTATIFS ZERO-LAG:
    ‚îú‚îÄ KAMA (Kaufman Adaptive MA)        - Le plus robuste
    ‚îú‚îÄ HMA (Hull MA)                      - Le plus rapide
    ‚îú‚îÄ Ehlers SuperSmoother              - Le plus pr√©cis
    ‚îú‚îÄ Ehlers Decycler                   - Suppression bruit
    ‚îú‚îÄ Ensemble (moyenne des 4)          - Robustesse max
    ‚îî‚îÄ üî• Efficiency Ratio (vitesse alpha) - Feature critique

LABELS (Y) - NON-CAUSALES (INCHANG√â):
‚îî‚îÄ filtfilt (Butterworth) sur RSI        - Cible id√©ale
```

### Filtres Impl√©ment√©s

**1. KAMA - Kaufman's Adaptive Moving Average ‚≠ê**
```python
# Efficiency Ratio (ER)
ER = |Prix[t] - Prix[t-10]| / Œ£|Prix[i] - Prix[i-1]|

# ER proche de 1 ‚Üí Tendance forte ‚Üí Filtre rapide
# ER proche de 0 ‚Üí Consolidation ‚Üí Filtre lent

# Feature CRITIQUE: filter_reactivity = ER
# Si ER devient soudainement √©lev√© ‚Üí Explosion volatilit√© imminente
```

**2. HMA - Hull Moving Average ‚ö°**
- D√©tecte les retournements AVANT les MA classiques
- Lag de phase minimal

**3. Ehlers SuperSmoother üéØ**
- Supprime bruit sans d√©caler la tendance
- Optimal pour CNN-LSTM

**4. Ehlers Decycler üîÑ**
- Isole la tendance pure
- Supprime cycles courts

### Fichiers

- `src/adaptive_filters.py` - Impl√©mentation 4 filtres + validation causalit√©
- `src/adaptive_features.py` - Integration au pipeline
- `SPEC_MISE_A_JOUR_FILTRES_ADAPTATIFS.md` - Doc compl√®te √©quipe dev

### Utilisation

```python
from adaptive_features import add_adaptive_filter_features

# Ajouter filtres adaptatifs sur prix
df = add_adaptive_filter_features(
    df,
    source_col='current_5m_close',
    filters=['kama', 'hma', 'supersmoother', 'decycler', 'ensemble'],
    add_slopes=True,        # Pentes des filtres
    add_reactivity=True     # Efficiency Ratio
)

# Features cr√©√©es:
# - kama_filtered, kama_slope
# - hma_filtered, hma_slope
# - supersmoother_filtered, supersmoother_slope
# - decycler_filtered, decycler_slope
# - ensemble_filtered, ensemble_slope
# - filter_reactivity ‚≠ê (vitesse du march√©)
```

### ‚ö†Ô∏è AVERTISSEMENT CRITIQUE

**INTERDICTION ABSOLUE: Fen√™tres centr√©es**

```python
# ‚ùå JAMAIS FAIRE:
df['ma'] = df['close'].rolling(window=10, center=True).mean()

# ‚úÖ TOUJOURS:
df['ma'] = df['close'].rolling(window=10, center=False).mean()
```

**Pourquoi?** `center=True` utilise le FUTUR = Data leakage = Accuracy artificielle 98%+

**D√©tection:** Si accuracy saute soudainement √† 98%+, chercher fen√™tres centr√©es!

### Validation Obligatoire

Avant chaque utilisation:

```python
from adaptive_filters import validate_causality

# V√©rifier que le filtre est causal
result = validate_causality(signal, kama_filter)
assert result['is_causal'], "Filtre non-causal d√©tect√©!"
```

### Impact Attendu

| M√©trique | Sans Filtres Adaptatifs | Avec Filtres Adaptatifs |
|----------|------------------------|-------------------------|
| Accuracy test | 75-80% | 85-92% ‚≠ê |
| Lag moyen | Moyen | Minimal |
| Features | ~15 | ~30 |
| Robustesse | Bonne | Excellente |

### R√©f√©rences Litt√©rature

- Kaufman (1995) - KAMA original
- Ehlers (2001, 2013) - SuperSmoother, Decycler
- Hull (2005) - Hull MA
- Renaissance Technologies - Multi-asset strategies

**Lire documentation compl√®te:** `SPEC_MISE_A_JOUR_FILTRES_ADAPTATIFS.md`

---

## üö® R√àGLE CRITIQUE: Trim des Bords de Filtres

### Probl√®me

**Les filtres ont besoin de warm-up (d√©but) et produisent des artifacts (fin).**

Les tests empiriques sur 200 points avec KAMA ont d√©montr√©:

| Zone | Index | Erreur Absolue | √âtat |
|------|-------|----------------|------|
| **D√âBUT** (warm-up) | 0-30 | **569.44** | ‚ùå √âLEV√âE |
| **MILIEU** (zone propre) | 30-170 | **488.31** | ‚úÖ FAIBLE |
| **FIN** (artifacts) | 170-200 | **349.42** | ‚ùå √âLEV√âE |

**Conclusion:** Les bords du dataset filtr√© sont IMPROPRES √† l'entra√Ænement.

### Solution Obligatoire

**‚ö†Ô∏è Enlever 30 valeurs au d√©but ET √† la fin AVANT de cr√©er train/val/test:**

```python
from utils import trim_filter_edges

# Workflow complet
df = load_ohlcv_data(filepath)                    # 1. Charger
df = create_ghost_candles(df)                     # 2. Ghost candles
df = add_advanced_features(df)                    # 3. Features
df = add_adaptive_filter_features(df)             # 4. Filtres adaptatifs
df = add_indicators(df)                           # 5. Indicateurs
df = add_labels(df)                               # 6. Labels

# ‚ö†Ô∏è CRITIQUE: Trim AVANT split
df_clean = trim_filter_edges(df, n_trim=30)      # 7. TRIM ‚Üê ICI!

# Maintenant cr√©er les splits
train, val, test = split_train_val_test(df_clean) # 8. Split
```

### Preuve Empirique

Les tests de validation ont g√©n√©r√© des visualisations d√©montrant l'effet:

- **`tests/validation_output/03_filter_edge_effects.png`** - Graphique prouvant les zones √† √©viter
- 3 graphiques: Signal complet, Erreur de filtrage, Zoom sur warm-up

### Fonction Utilitaire

```python
def trim_filter_edges(df: pd.DataFrame,
                      n_trim: int = 30,
                      timestamp_col: str = 'timestamp') -> pd.DataFrame:
    """
    Enl√®ve les bords apr√®s filtrage (warm-up + artifacts).

    Returns: DataFrame sans les n_trim premi√®res et derni√®res lignes

    Raises: ValueError si dataset trop petit
    """
```

### Dimensionnement

| Taille Dataset | n_trim Recommand√© | Dataset Final |
|----------------|-------------------|---------------|
| < 200 | ‚ö†Ô∏è Trop petit | N/A |
| 200-500 | 20 | 160-460 |
| 500-2000 | 30 | 440-1940 |
| 2000-10000 | 50 | 1900-9900 |
| > 10000 | 100 | > 9800 |

### ‚ö†Ô∏è Checklist Avant Entra√Ænement

- [ ] ‚úÖ Filtres appliqu√©s (KAMA, HMA, etc.)
- [ ] ‚úÖ Indicateurs calcul√©s (RSI, CCI, etc.)
- [ ] ‚úÖ Labels cr√©√©s (pente filtr√©e)
- [ ] ‚úÖ **TRIM effectu√© (30 valeurs d√©but + fin)**
- [ ] ‚úÖ Split train/val/test cr√©√© APR√àS trim
- [ ] ‚úÖ Validation data leakage effectu√©e

**Documentation compl√®te:** `REGLES_CRITIQUES_FILTRES.md`

---

## Spec #2 : Architecture IA (Phase Future)

### Mod√®le Hybride CNN-LSTM ou TCN

1. **Couche Convolutionnelle (CNN)** : Extraire motifs sur 10 derni√®res bougies
2. **Couche R√©currente (LSTM)** : M√©moriser s√©quence des steps (5m, 10m, 15m...)
3. **Couche Dense (Stacking)** : Combiner probabilit√©s ‚Üí d√©cision finale (0 ou 1)

---

## M√©thodologie de D√©veloppement

### Phase 1 : Pipeline de Donn√©es ‚úÖ (En cours)
1. Cr√©er scripts de transformation 5m ‚Üí 30m
2. G√©n√©rer dataset avec bougie fant√¥me
3. Calculer indicateurs + normalisation
4. Calculer labels (pente filtr√©e d√©cal√©e)
5. Valider dataset (pas de leakage, distribution labels)

### Phase 2 : Mod√®le IA (√Ä venir)
1. Architecture CNN-LSTM/TCN
2. Entra√Ænement avec GPU
3. Validation crois√©e temporelle
4. Backtesting

---

## Commandes Utiles

### V√©rifier GPU
```python
# TensorFlow
import tensorflow as tf
print(tf.config.list_physical_devices('GPU'))

# PyTorch
import torch
print(torch.cuda.is_available())
```

### Lancer Pipeline
```bash
python src/data_pipeline.py --input ../data_trad/BTCUSD_all_5m.csv --output data/processed/btc_30m_dataset.csv
```

---

## Notes pour Claude

- **GPU FIRST** : Toujours v√©rifier et utiliser le GPU
- **DRY Principle** : Don't Repeat Yourself - Partager le code via utils.py
- **Data Integrity** : V√©rifier SYST√âMATIQUEMENT qu'il n'y a pas de data leakage
- **Validation** : Chaque √©tape doit √™tre valid√©e avant de passer √† la suivante
- Objectif Accuracy > 90% n√©cessite une **qualit√© de donn√©es parfaite**

---

## Licence

Ce projet est sous licence Unlicense - voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.
