#!/usr/bin/env python3
"""Script de diagnostic pour vÃ©rifier le dataset Direction-Only."""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

import numpy as np

def diagnose_dataset(npz_path: str):
    """Diagnostique le dataset et affiche les statistiques."""

    print("="*80)
    print("DIAGNOSTIC DU DATASET")
    print("="*80)

    # Charger le fichier brut (sans extraction)
    data = np.load(npz_path, allow_pickle=True)

    print(f"\nğŸ“‚ Fichier: {npz_path}")
    print(f"\nğŸ”‘ ClÃ©s disponibles: {list(data.keys())}")

    # Analyser X_train
    X_train = data['X_train']
    Y_train = data['Y_train']

    print(f"\nğŸ“Š Shapes:")
    print(f"  X_train: {X_train.shape}")
    print(f"  Y_train: {Y_train.shape}")

    # Analyser le contenu de X (premiÃ¨re sÃ©quence, premier timestep)
    print(f"\nğŸ” Contenu de X (premiÃ¨re sÃ©quence, timestep 0):")
    print(f"  Feature 0: {X_train[0, 0, 0]:.6f}")
    print(f"  Feature 1: {X_train[0, 0, 1]:.6f}")
    print(f"  Feature 2: {X_train[0, 0, 2]:.6f}")

    # VÃ©rifier si features 0 et 1 sont timestamp et asset_id
    print(f"\nğŸ” Analyse des features:")
    print(f"  Feature 0 min/max: {X_train[:, 0, 0].min():.2f} / {X_train[:, 0, 0].max():.2f}")
    print(f"  Feature 1 min/max: {X_train[:, 0, 1].min():.2f} / {X_train[:, 0, 1].max():.2f}")
    print(f"  Feature 2 min/max: {X_train[:, 0, 2].min():.6f} / {X_train[:, 0, 2].max():.6f}")

    # Si feature 1 ressemble Ã  asset_id
    if np.all(X_train[:, 0, 1] >= 0) and np.all(X_train[:, 0, 1] <= 10):
        print(f"  âš ï¸  Feature 1 ressemble Ã  asset_id (0-4)")

    # Si feature 2 ressemble Ã  un return
    if np.abs(X_train[:, 0, 2]).max() < 1.0:
        print(f"  âœ… Feature 2 ressemble Ã  un return (-1 Ã  +1)")

    # Asset IDs uniques dans X
    asset_ids_x = np.unique(X_train[:, 0, 1])
    print(f"\nğŸ¯ Asset IDs uniques dans X[:, 0, 1]: {asset_ids_x}")
    print(f"  Nombre d'assets: {len(asset_ids_x)}")

    # Si Y a 3 colonnes, vÃ©rifier asset_ids dans Y
    if Y_train.ndim == 2 and Y_train.shape[1] == 3:
        asset_ids_y = np.unique(Y_train[:, 1])
        print(f"\nğŸ¯ Asset IDs uniques dans Y[:, 1]: {asset_ids_y}")
        print(f"  Nombre d'assets: {len(asset_ids_y)}")

        # VÃ©rifier si X et Y ont les mÃªmes asset_ids
        if np.array_equal(asset_ids_x, asset_ids_y):
            print("  âœ… Asset IDs cohÃ©rents entre X et Y")
        else:
            print("  âš ï¸ INCOHÃ‰RENCE entre X et Y!")

    # Distribution des labels (colonne 2 si Y a 3 colonnes)
    if Y_train.ndim == 2 and Y_train.shape[1] == 3:
        labels = Y_train[:, 2]
    else:
        labels = Y_train.flatten()

    print(f"\nğŸ“Š Distribution des labels:")
    unique_labels, counts = np.unique(labels, return_counts=True)
    for label, count in zip(unique_labels, counts):
        pct = count / len(labels) * 100
        print(f"  Label {label}: {count:,} ({pct:.1f}%)")

    # VÃ©rifier les valeurs invalides
    print(f"\nğŸ” VÃ©rifications:")
    print(f"  NaN dans X: {np.isnan(X_train).any()}")
    print(f"  NaN dans Y: {np.isnan(Y_train).any()}")
    print(f"  Inf dans X: {np.isinf(X_train).any()}")
    print(f"  Inf dans Y: {np.isinf(Y_train).any()}")

    # Compter les sÃ©quences par asset
    print(f"\nğŸ“Š SÃ©quences par asset (depuis X):")
    for asset_id in sorted(asset_ids_x):
        count = np.sum(X_train[:, 0, 1] == asset_id)
        pct = count / len(X_train) * 100
        print(f"  Asset ID {int(asset_id)}: {count:,} sÃ©quences ({pct:.1f}%)")

    # Metadata
    if 'metadata' in data:
        metadata = data['metadata'].item()
        print(f"\nğŸ“‹ Metadata:")
        if 'assets' in metadata:
            print(f"  Assets: {metadata['assets']}")
        if 'features' in metadata:
            print(f"  Features: {metadata['features']}")
        if 'labels' in metadata:
            print(f"  Labels: {metadata['labels']}")

    print("\n" + "="*80)
    print("FIN DU DIAGNOSTIC")
    print("="*80)

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', required=True, help='Chemin du dataset .npz')
    args = parser.parse_args()

    diagnose_dataset(args.data)
