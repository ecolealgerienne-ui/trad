# Approche IA : PrÃ©diction de la Pente des Filtres

## âš ï¸ IMPORTANT : Clarification de l'Approche

### Ce qu'on NE fait PAS
âŒ **On n'utilise PAS les filtres non-causaux en production**
âŒ **On ne prÃ©dit PAS les valeurs exactes des filtres**
âŒ **On ne cherche PAS Ã  reproduire smooth/filtfilt**

### Ce qu'on FAIT
âœ… **On prÃ©dit la PENTE (direction) du filtre entre t-1 et t-2**
âœ… **Classification binaire : 1 si filtre[t-1] > filtre[t-2], sinon 0**
âœ… **L'IA apprend Ã  dÃ©tecter les changements de direction**

---

## Objectif de l'IA (CNN-LSTM)

### EntrÃ©e (Features X)
- Ghost Candles (bougies fantÃ´mes 30min Ã©chantillonnÃ©es Ã  5min)
- OHLCV 5min
- Indicateurs techniques (RSI, Volume, etc.)
- **Toutes les features sont CAUSALES** (pas de futur)

### Sortie (Label Y)
```python
# Label binaire : Direction de la pente du filtre
Y[t] = 1  si  filter[t-1] > filter[t-2]  # Pente haussiÃ¨re â†’ BUY
Y[t] = 0  si  filter[t-1] <= filter[t-2] # Pente baissiÃ¨re â†’ SELL
```

**Type de problÃ¨me**: Classification binaire
**Activation finale**: Sigmoid
**Loss function**: Binary Cross-Entropy

---

## Pipeline Complet

### Phase 1 : GÃ©nÃ©ration des Labels (Offline)

```python
# 1. Calculer le filtre de rÃ©fÃ©rence (NON-CAUSAL pour crÃ©er les labels)
#    On utilise Kalman smooth ou autre filtre parfait pour crÃ©er les VRAIS labels
filtered_reference = kalman_filter(close, smooth=True)

# 2. GÃ©nÃ©rer les labels de pente
labels = np.zeros(len(filtered_reference))
for t in range(2, len(filtered_reference)):
    if filtered_reference[t-1] > filtered_reference[t-2]:
        labels[t] = 1  # Pente haussiÃ¨re
    else:
        labels[t] = 0  # Pente baissiÃ¨re

# 3. Ces labels servent Ã  ENTRAÃNER le modÃ¨le
```

### Phase 2 : EntraÃ®nement du ModÃ¨le

```python
# Le modÃ¨le apprend Ã  prÃ©dire la pente Ã  partir des features causales
model = CNN_LSTM()

# Features : Ghost Candles + OHLCV (CAUSALES)
X_train = ghost_candles[:-trim]

# Labels : Pente du filtre parfait (gÃ©nÃ©rÃ©s offline)
Y_train = labels[:-trim]

model.fit(X_train, Y_train)
```

### Phase 3 : Utilisation en Production (Online)

```python
# En temps rÃ©el, on prÃ©dit la pente directement
prediction = model.predict(ghost_candles_current)

# prediction = probabilitÃ© que filter[t-1] > filter[t-2]
if prediction > 0.5:
    signal = 'BUY'   # On prÃ©dit pente haussiÃ¨re
    position = 1
else:
    signal = 'SELL'  # On prÃ©dit pente baissiÃ¨re
    position = -1

# Trade Ã  open[t+1]
trade_price = open[t+1]
```

---

## Pourquoi cette Approche ?

### Avantages

1. **SimplicitÃ©**
   - Classification binaire (plus simple que rÃ©gression)
   - Pas besoin de prÃ©dire valeurs exactes du filtre
   - Juste prÃ©dire la direction (hausse/baisse)

2. **Robustesse**
   - Moins sensible aux outliers
   - Classification est plus stable que rÃ©gression
   - MÃ©triques claires (Accuracy, Precision, Recall)

3. **RÃ©alisme**
   - On prÃ©dit seulement ce dont on a besoin (direction)
   - Pas de sur-engineering
   - Compatible avec trading rÃ©el

4. **Performance**
   - ModÃ¨le plus lÃ©ger (classification vs rÃ©gression)
   - Inference plus rapide
   - Moins de paramÃ¨tres Ã  optimiser

### Lien avec les Tests "Monde Parfait"

Les tests avec filtres non-causaux (`test_perfect_world.py`) servent Ã  :

âœ… **VALIDER** que la mÃ©thode `filter[t-1] > filter[t-2]` fonctionne
âœ… **PROUVER** qu'avec un filtre exact, on obtient Profit Factor > 7.44
âœ… **JUSTIFIER** pourquoi on investit dans l'IA pour prÃ©dire cette pente

**Mais en production**, on n'utilise PAS les filtres non-causaux. On utilise l'IA qui prÃ©dit la pente.

---

## MÃ©triques de SuccÃ¨s de l'IA

### Pendant l'EntraÃ®nement

```python
# Classification metrics
accuracy = (predictions == labels).mean()
precision = TP / (TP + FP)
recall = TP / (TP + FN)
f1_score = 2 * (precision * recall) / (precision + recall)
```

**Target** : Accuracy > 55-60% (au-dessus du hasard 50%)

### En Backtesting

```python
# Trading metrics (avec les prÃ©dictions de l'IA)
profit_factor = gross_profit / gross_loss
win_rate = winning_trades / total_trades
sharpe_ratio = mean_return / std_return * sqrt(252)
```

**Target** : Profit Factor > 1.5-2.0 (rÃ©aliste avec IA)

âš ï¸ **On ne vise PAS 7.44** (monde parfait) mais quelque chose de rÃ©aliste !

---

## Architecture du ModÃ¨le (Exemple)

```python
class PentePredictorCNN_LSTM(nn.Module):
    """
    PrÃ©dit la pente du filtre : 1 si hausse, 0 si baisse.
    """
    def __init__(self):
        super().__init__()

        # CNN pour extraire features locales
        self.conv1 = nn.Conv1d(in_channels=5, out_channels=64, kernel_size=3)
        self.conv2 = nn.Conv1d(64, 128, kernel_size=3)

        # LSTM pour capturer dynamique temporelle
        self.lstm = nn.LSTM(input_size=128, hidden_size=64, num_layers=2)

        # Classification binaire
        self.fc = nn.Linear(64, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # x shape: (batch, sequence, features)
        x = x.transpose(1, 2)  # (batch, features, sequence)

        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))

        x = x.transpose(1, 2)  # (batch, sequence, features)

        out, _ = self.lstm(x)

        # Prendre derniÃ¨re sortie
        out = out[:, -1, :]

        # Classification
        out = self.fc(out)
        out = self.sigmoid(out)

        return out  # ProbabilitÃ© de pente haussiÃ¨re
```

**Loss function** :
```python
criterion = nn.BCELoss()  # Binary Cross-Entropy
```

---

## DiffÃ©rence Clef : Monde Parfait vs Production

| Aspect | Monde Parfait (Validation) | Production (IA) |
|--------|---------------------------|-----------------|
| **Objectif** | Valider la mÃ©thode thÃ©oriquement | Trading rÃ©el |
| **Filtre** | Non-causal (smooth/filtfilt) | Pas de filtre ! IA directe |
| **PrÃ©diction** | Valeurs exactes du filtre | Pente (direction) 0/1 |
| **Profit Factor** | 7.44 - 995 (thÃ©orique) | 1.5 - 3.0 (rÃ©aliste) |
| **Utilisation** | Proof of concept | SystÃ¨me de trading |

---

## Workflow Complet

### 1. PrÃ©paration des DonnÃ©es
```python
# GÃ©nÃ©rer Ghost Candles (features causales)
ghost_candles = create_ghost_candles(df_5m, target_timeframe='30min')

# GÃ©nÃ©rer labels (avec filtre parfait OFFLINE)
filtered = kalman_filter(df_5m['close'], smooth=True)  # Non-causal OK ici
labels = (filtered[1:] > filtered[:-1]).astype(int)

# Split train/val/test (avec trim des edges !)
X_train, Y_train = trim_and_split(ghost_candles, labels, trim=100)
```

### 2. EntraÃ®nement
```python
model = PentePredictorCNN_LSTM()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.BCELoss()

for epoch in range(epochs):
    predictions = model(X_train)
    loss = criterion(predictions, Y_train)

    loss.backward()
    optimizer.step()

    # MÃ©triques
    accuracy = ((predictions > 0.5) == Y_train).float().mean()
    print(f"Epoch {epoch}: Loss={loss:.4f}, Accuracy={accuracy:.2%}")
```

### 3. Backtesting avec IA
```python
# En backtest, utiliser les PRÃ‰DICTIONS de l'IA
predictions = model.predict(X_test)

signals = []
for t in range(2, len(predictions)):
    if predictions[t] > 0.5:
        signal = 'BUY'
    else:
        signal = 'SELL'
    signals.append(signal)

# Tester la stratÃ©gie avec ces signaux
results = backtest_strategy(df_test, signals)
print(f"Profit Factor: {results['profit_factor']:.2f}")
```

---

## RÃ©sumÃ©

ğŸ¯ **Objectif IA** : PrÃ©dire si `filter[t-1] > filter[t-2]` (classification binaire)

âœ… **EntrÃ©e** : Ghost Candles + features causales
âœ… **Sortie** : 0 ou 1 (pente baisse ou hausse)
âœ… **EntraÃ®nement** : Labels gÃ©nÃ©rÃ©s avec filtre parfait (offline)
âœ… **Production** : IA directe, pas de filtre

âŒ **On n'utilise PAS** les filtres non-causaux en production
âŒ **On ne prÃ©dit PAS** les valeurs du filtre
âŒ **On ne vise PAS** Profit Factor 7.44 (irrÃ©aliste avec IA)

ğŸ“š **RÃ©fÃ©rence** : Les tests "monde parfait" valident la mÃ©thode, mais le systÃ¨me rÃ©el utilise l'IA pour prÃ©dire la pente.

---

**Date** : 2026-01-01
**Version** : 1.0
**Status** : Approche validÃ©e et documentÃ©e
