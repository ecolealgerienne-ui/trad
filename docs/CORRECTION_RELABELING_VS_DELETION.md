# CORRECTION CRITIQUE: Relabeling vs Suppression

**Date**: 2026-01-06
**Contexte**: Correction de l'approche Phase 1 suite Ã  feedback utilisateur
**Statut**: âœ… **RELABELING (Target Correction) validÃ© comme approche correcte**

---

## ğŸš¨ ProblÃ¨me IdentifiÃ© avec la Suppression

### L'Approche Initiale (Experts) - INCORRECTE

**Expert 1 et 2 avaient recommandÃ©**: Supprimer les samples "Kill Zone" (Duration 3-5) et Vol Q4.

**ProblÃ¨me critique soulevÃ© par l'utilisateur**:

> "Supprimer les donnÃ©es 'difficiles' revient Ã  mettre des Å“illÃ¨res au modÃ¨le.
>
> Si tu les supprimes du Train : Le modÃ¨le ne voit jamais ces piÃ¨ges.
>
> En Prod : Il tombe dedans la tÃªte la premiÃ¨re car il ne sait pas que ce sont des piÃ¨ges."

---

## âŒ Pourquoi la Suppression est Dangereuse

### ScÃ©nario Catastrophe

```
TRAINING (avec suppression):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Dataset: Tous les "piÃ¨ges" (Duration 3-5, Vol Q4) SUPPRIMÃ‰S

Le modÃ¨le apprend dans un monde "propre":
  X: [Patterns faciles, belles tendances]
  Y: Force=STRONG (toujours profitable)

Le modÃ¨le pense: "Si X ressemble Ã  Ã§a â†’ Force=STRONG â†’ Profitable âœ…"

Accuracy train: 95% (excellent!)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

PRODUCTION (rÃ©alitÃ©):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Le modÃ¨le rencontre: Duration=4, Vol=Haute (piÃ¨ge classique)

  X: [Pattern qui RESSEMBLE Ã  STRONG]
  ModÃ¨le prÃ©dit: Force=STRONG (car il n'a jamais vu ce piÃ¨ge!)

Action: LONG
RÃ©sultat: PERTE (-2%)

Le modÃ¨le pense: "Mais... je n'ai JAMAIS vu Ã§a en training! ğŸ˜±"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

CATASTROPHE:
  - Accuracy train: 95% âœ…
  - Accuracy prod: 60% âŒ (car rencontre des piÃ¨ges non vus)
  - Win Rate: 30% âŒ (tombe dans tous les Bull Traps)
```

**Conclusion**: Suppression = **Overfitting sur un monde trop facile**

---

## âœ… La Solution Professionnelle: RELABELING

### Principe (Target Correction / Hard Negative Mining)

**Au lieu de cacher les piÃ¨ges, on les MONTRE au modÃ¨le et on lui DIT que ce sont des piÃ¨ges.**

```python
# AVANT (suppression) - âŒ MAUVAIS
if duration in [3, 4, 5]:
    # Supprimer ce sample du dataset
    continue

# APRÃˆS (relabeling) - âœ… CORRECT
if duration in [3, 4, 5]:
    # Garder le sample MAIS relabeler
    Y[i, 1] = 0  # Force: STRONG â†’ WEAK
    # "Attention modÃ¨le: cette config RESSEMBLE Ã  STRONG mais c'est WEAK!"
```

---

## ğŸ¯ Hard Negative Mining - Technique ML Classique

### Ce que c'est

**Hard Negative Mining**: Technique oÃ¹ on force le modÃ¨le Ã  apprendre sur les **exemples difficiles**.

**Dans notre cas**:
- **Hard Negatives**: Configurations qui RESSEMBLENT Ã  STRONG mais sont en rÃ©alitÃ© WEAK (piÃ¨ges)
- **Mining**: On les identifie (Duration 3-5, Vol Q4)
- **Relabeling**: On force Y=0 (WEAK) pour que le modÃ¨le apprenne Ã  les reconnaÃ®tre

### LittÃ©rature ML

| Technique | RÃ©fÃ©rence | Application |
|-----------|-----------|-------------|
| **Hard Negative Mining** | Felzenszwalb et al. (2010) - Object Detection | Apprendre Ã  rejeter faux positifs |
| **Target Correction** | Patrini et al. (2017) - Noisy Labels | Corriger labels bruitÃ©s |
| **Curriculum Learning** | Bengio et al. (2009) | Apprendre exemples difficiles |

**Notre cas**: Combinaison de Hard Negative Mining + Target Correction

---

## ğŸ“Š Comparaison Suppression vs Relabeling

### ScÃ©nario: 10,000 samples, 14% sont des piÃ¨ges (Duration 3-5)

#### Approche 1: SUPPRESSION (âŒ)

```
TRAINING:
  Samples total: 8,600 (1,400 piÃ¨ges supprimÃ©s)

  Le modÃ¨le voit:
    X: [Patterns faciles uniquement]
    Y: Force=STRONG (toujours profitable)

  Accuracy train: 95%
  Le modÃ¨le pense: "Je suis excellent!"

PRODUCTION:
  Le modÃ¨le rencontre: 14% de piÃ¨ges (comme dans la vraie vie)

  PrÃ©diction: Force=STRONG (car jamais vu ces configs!)

  RÃ©sultat:
    - Sur "vrais STRONG" (86%): 95% accuracy âœ…
    - Sur "piÃ¨ges" (14%): 10% accuracy âŒ (alÃ©atoire!)

  Accuracy prod globale: 86% Ã— 0.95 + 14% Ã— 0.10 = 83%

  DÃ‰GRADATION: -12% (95% â†’ 83%)
```

#### Approche 2: RELABELING (âœ…)

```
TRAINING:
  Samples total: 10,000 (AUCUN supprimÃ©)

  Le modÃ¨le voit:
    X: [Patterns faciles + piÃ¨ges]
    Y: Force=STRONG pour vrais STRONG
        Force=WEAK pour piÃ¨ges (relabelÃ©s!)

  Le modÃ¨le APPREND:
    "Cette config (Duration=4) RESSEMBLE Ã  STRONG mais â†’ prÃ©dit WEAK"
    "Cette config (Vol=Haute) est instable â†’ prÃ©dit WEAK"

  Accuracy train: 90% (plus difficile, mais HONNÃŠTE)

PRODUCTION:
  Le modÃ¨le rencontre: 14% de piÃ¨ges

  PrÃ©diction: Force=WEAK (car A APPRIS Ã  les dÃ©tecter!)

  RÃ©sultat:
    - Sur "vrais STRONG" (86%): 90% accuracy âœ…
    - Sur "piÃ¨ges" (14%): 85% accuracy âœ… (DÃ‰TECTE!)

  Accuracy prod globale: 86% Ã— 0.90 + 14% Ã— 0.85 = 89%

  AMÃ‰LIORATION: -1% (90% â†’ 89%) - STABLE!
```

**Verdict**: Relabeling gÃ©nÃ©ralise **BEAUCOUP mieux** (+6% vs suppression en prod)

---

## ğŸ§  Pourquoi le Deep Learning Brille Ici

### Le DÃ©fi: Apprendre des Distinctions Subtiles

**Le modÃ¨le va voir des X qui se ressemblent beaucoup**:

```python
Sample A (Vrai STRONG):
  X: [VolatilitÃ©=0.5%, Duration=7, Range=Small, Trend=Up]
  Y: Force=1 (STRONG) âœ…

Sample B (PiÃ¨ge - Duration courte):
  X: [VolatilitÃ©=0.6%, Duration=4, Range=Small, Trend=Up]
  Y: Force=0 (WEAK) â† RelabelÃ©!

Sample C (PiÃ¨ge - Vol haute):
  X: [VolatilitÃ©=2.3%, Duration=7, Range=Large, Trend=Up]
  Y: Force=0 (WEAK) â† RelabelÃ©!
```

**Le CNN-LSTM doit apprendre**:
- "Si Duration < 6 â†’ probablement WEAK (mÃªme si Ã§a monte)"
- "Si Vol > seuil â†’ probablement WEAK (trop instable)"
- "Si les deux sont OK â†’ STRONG"

**C'est exactement ce pour quoi le Deep Learning est fait!**

---

## ğŸ“ Citation Utilisateur (Validation)

> "Ta proposition de 'Changer la classe' (Relabeling) est la seule approche professionnelle. C'est ce qu'on appelle en Machine Learning du **Target Correction** ou du **Hard Negative Mining**."

> "Apprentissage Difficile (Hard Learning): Le modÃ¨le va voir des X qui se ressemblent beaucoup. Certains ont Y=1 (DurÃ©e > 6), d'autres Y=0 (DurÃ©e 3-5). Il va devoir creuser profond pour trouver la diffÃ©rence subtile. **C'est lÃ  que le Deep Learning brille.**"

---

## ğŸ”„ Changement de la "Question de l'Examen"

### Clarification Importante

**L'utilisateur prÃ©cise**:
> "Pas de Triche sur le Test : En changeant le label Y du Test, on ne change pas les donnÃ©es du test (X), on change la **Question de l'examen**."

**Avant Relabeling**:
```
Question: "Est-ce que le Kalman monte ?"
RÃ©ponse: Oui (accuracy 92%)
Mais: On perd de l'argent (Win Rate 14%)
```

**AprÃ¨s Relabeling**:
```
Question: "Est-ce que c'est une tendance SAINE ?"
RÃ©ponse: Non si Duration=4 ou Vol=Haute (c'est un piÃ¨ge)
RÃ©sultat: Accuracy peut monter ET PnL aussi
```

**Ce n'est PAS de la triche**, c'est **corriger l'objectif d'apprentissage**.

---

## ğŸ“ Nouveau Script: relabel_dataset_phase1.py

### DiffÃ©rences ClÃ©s vs Script de Suppression

| Aspect | Suppression (âŒ) | Relabeling (âœ…) |
|--------|------------------|-----------------|
| **Samples totaux** | RÃ©duits (~14-24%) | INCHANGÃ‰S (100%) |
| **Labels Force** | SupprimÃ©s (piÃ¨ges absents) | RelabelÃ©s (1â†’0 pour piÃ¨ges) |
| **X (features)** | RÃ©duits | INCHANGÃ‰S |
| **Le modÃ¨le voit** | Monde "facile" | Monde RÃ‰EL (avec piÃ¨ges) |
| **En production** | Surpris par piÃ¨ges | DÃ‰TECTE les piÃ¨ges |
| **GÃ©nÃ©ralisation** | âŒ Overfitting facile | âœ… Robuste |

### Logique du Script

```python
# 1. Identifier les piÃ¨ges
mask_duration_trap = np.isin(duration, [3, 4, 5])
mask_vol_trap = (vol > q4_threshold) if indicator in ['macd', 'cci'] else False
mask_trap = mask_duration_trap | mask_vol_trap

# 2. RELABELING (PAS DE SUPPRESSION!)
Y_relabeled = Y.copy()
for i in np.where(mask_trap)[0]:
    if Y[i, 1] == 1:  # Si c'Ã©tait STRONG
        Y[i, 1] = 0   # â†’ Forcer WEAK (apprendre que c'est un piÃ¨ge)

# 3. X RESTE INCHANGÃ‰ (le modÃ¨le voit tout)
data_relabeled = {
    'X_train': X_train,           # INCHANGÃ‰
    'Y_train': Y_train_relabeled  # RELABELÃ‰
}
```

---

## ğŸ¯ Gains Attendus (Relabeling vs Suppression)

### Suppression (Experts - Incorrect)

```
Gain attendu: +5-8% Oracle accuracy
ProblÃ¨me: NE gÃ©nÃ©ralise PAS en prod (overfitting sur monde facile)

RÃ©sultat rÃ©el attendu:
  - Train: +8% âœ…
  - Prod: -5% âŒ (tombe dans piÃ¨ges non vus)
```

### Relabeling (Utilisateur - Correct)

```
Gain attendu: +3-5% accuracy (plus conservateur mais HONNÃŠTE)
Avantage: GÃ‰NÃ‰RALISE en prod (le modÃ¨le connaÃ®t les piÃ¨ges)

RÃ©sultat rÃ©el attendu:
  - Train: +4% âœ… (plus difficile, mais robuste)
  - Prod: +4% âœ… (STABLE - pas de surprise)
  - Win Rate: +8-12% (dÃ©tecte les faux STRONG)
```

---

## ğŸš€ Prochaines Ã‰tapes (CorrigÃ©es)

### âŒ NE PAS FAIRE

~~1. ExÃ©cuter `clean_dataset_phase1.py` (suppression)~~
~~2. RÃ©entraÃ®ner sur datasets `_cleaned.npz`~~

### âœ… FAIRE

1. **ExÃ©cuter `relabel_dataset_phase1.py`** (relabeling):
   ```bash
   python src/relabel_dataset_phase1.py --assets BTC ETH BNB ADA LTC
   ```

2. **RÃ©entraÃ®ner sur datasets `_relabeled.npz`**:
   ```bash
   python src/train.py --data data/prepared/dataset_*_macd_*_relabeled.npz --epochs 50
   python src/train.py --data data/prepared/dataset_*_rsi_*_relabeled.npz --epochs 50
   python src/train.py --data data/prepared/dataset_*_cci_*_relabeled.npz --epochs 50
   ```

3. **Ã‰valuer et comparer**:
   ```bash
   python src/evaluate.py --data data/prepared/dataset_*_macd_*_relabeled.npz
   ```

4. **Backtest avec nouveaux modÃ¨les** (attendu: Win Rate +8-12%)

---

## ğŸ“š RÃ©fÃ©rences ML

| Technique | Papier | AnnÃ©e | Application |
|-----------|--------|-------|-------------|
| **Hard Negative Mining** | Felzenszwalb et al. | 2010 | Object detection (rejeter faux positifs) |
| **Target Correction** | Patrini et al. | 2017 | Learning with noisy labels |
| **Curriculum Learning** | Bengio et al. | 2009 | Apprendre exemples difficiles progressivement |
| **Focal Loss** | Lin et al. | 2017 | PondÃ©rer exemples difficiles |

**Notre approche**: Hard Negative Mining + Target Correction = **Apprentissage robuste sur piÃ¨ges**

---

## ğŸ“ LeÃ§on Apprise

### Erreur des Experts

Les 2 experts ML finance ont recommandÃ© la **suppression** sans considÃ©rer l'impact en production.

**Pourquoi?**
- Focus sur "nettoyer les donnÃ©es" (vision batch ML classique)
- Pas assez d'attention sur la **gÃ©nÃ©ralisation en production**

### Correction de l'Utilisateur

L'utilisateur a identifiÃ© le problÃ¨me fondamental:

> "Si tu les supprimes du Train : Le modÃ¨le ne voit jamais ces piÃ¨ges.
> En Prod : Il tombe dedans la tÃªte la premiÃ¨re."

**C'est 100% correct** - une vision production-first.

---

## âœ… Conclusion

**RELABELING (Target Correction) est l'approche professionnelle correcte.**

**Avantages**:
1. âœ… Le modÃ¨le VOIT les piÃ¨ges
2. âœ… Il APPREND Ã  les reconnaÃ®tre
3. âœ… En prod, il les DÃ‰TECTE
4. âœ… GÃ©nÃ©ralisation robuste
5. âœ… Hard Learning â†’ Deep Learning brille

**La suppression Ã©tait une erreur** - merci Ã  l'utilisateur de l'avoir identifiÃ©e.

---

**Auteur**: Claude Code (correction par utilisateur)
**Date**: 2026-01-06
**Statut**: âœ… **RELABELING validÃ© comme approche correcte**
**Script**: `src/relabel_dataset_phase1.py`
