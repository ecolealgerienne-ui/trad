#!/usr/bin/env python3
"""
G√©n√©ration des M√©ta-Features pour Stacking / Ensemble Learning

Objectif: Combiner les pr√©dictions de 3 mod√®les experts (MACD, RSI, CCI)
pour am√©liorer la pr√©diction de Direction (Kalman original).

Pipeline:
  1. Charger les 3 mod√®les entra√Æn√©s (.pth)
  2. Charger les 3 datasets correspondants
  3. G√©n√©rer les pr√©dictions (probabilit√©s) pour Train/Val/Test
  4. Sauvegarder les m√©ta-features

Output:
  X_meta: (n, 6) - [p_macd_dir, p_macd_force, p_rsi_dir, p_rsi_force, p_cci_dir, p_cci_force]
  Y_meta: (n, 1) - Direction Kalman original (cible commune)

Usage:
  python src/generate_meta_features.py --assets BTC ETH BNB ADA LTC
"""

import numpy as np
import torch
from torch.utils.data import DataLoader, TensorDataset
from pathlib import Path
import logging
import argparse
from typing import Dict, Tuple

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

# Imports locaux
from constants import BATCH_SIZE
from model import create_model


def load_model_and_predict(
    model_path: str,
    X: np.ndarray,
    device: str,
    batch_size: int = 256
) -> np.ndarray:
    """
    Charge un mod√®le et g√©n√®re les pr√©dictions (probabilit√©s).

    Args:
        model_path: Chemin vers le mod√®le .pth
        X: Features (n, seq_len, n_features)
        device: Device ('cuda' ou 'cpu')
        batch_size: Taille des batchs

    Returns:
        Pr√©dictions (n, 2) - [proba_direction, proba_force]
    """
    # Charger checkpoint
    checkpoint = torch.load(model_path, map_location=device)
    model_config = checkpoint.get('model_config', {})

    # Cr√©er mod√®le
    n_features = X.shape[2]

    # Retirer n_features et num_outputs de model_config s'ils existent
    # (on les passe explicitement)
    model_config_clean = {k: v for k, v in model_config.items()
                          if k not in ['n_features', 'num_outputs']}

    model = create_model(
        n_features=n_features,
        num_outputs=2,  # Direction + Force
        **model_config_clean
    ).to(device)

    # Charger poids
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    logger.info(f"   Mod√®le charg√©: {Path(model_path).name}")
    logger.info(f"   Features: {n_features}, Outputs: 2")

    # Cr√©er DataLoader
    X_tensor = torch.FloatTensor(X)
    dataset = TensorDataset(X_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

    # Pr√©dictions
    all_predictions = []
    with torch.no_grad():
        for (X_batch,) in dataloader:
            X_batch = X_batch.to(device)
            # predict_proba retourne probabilit√©s (g√®re sigmoid automatiquement)
            probs = model.predict_proba(X_batch)  # (batch, 2)
            all_predictions.append(probs.cpu().numpy())

    predictions = np.vstack(all_predictions)  # (n, 2)
    logger.info(f"   Pr√©dictions g√©n√©r√©es: {predictions.shape}")

    return predictions


def generate_meta_features_for_split(
    split_name: str,
    models_paths: Dict[str, str],
    datasets_paths: Dict[str, str],
    device: str
) -> Tuple[np.ndarray, np.ndarray]:
    """
    G√©n√®re les m√©ta-features pour un split (train/val/test).

    Args:
        split_name: 'train', 'val', ou 'test'
        models_paths: {'macd': path, 'rsi': path, 'cci': path}
        datasets_paths: {'macd': path, 'rsi': path, 'cci': path}
        device: Device

    Returns:
        X_meta: (n, 6) - Pr√©dictions des 3 mod√®les
        Y_meta: (n, 1) - Direction Kalman (cible commune)
    """
    logger.info(f"\n{'='*80}")
    logger.info(f"G√©n√©ration M√©ta-Features: {split_name.upper()}")
    logger.info('='*80)

    predictions = {}
    Y_meta = None

    for indicator in ['macd', 'rsi', 'cci']:
        logger.info(f"\n{indicator.upper()}:")

        # Charger dataset
        data = np.load(datasets_paths[indicator], allow_pickle=True)
        X = data[f'X_{split_name}']
        Y = data[f'Y_{split_name}']

        logger.info(f"   Dataset: {Path(datasets_paths[indicator]).name}")
        logger.info(f"   X shape: {X.shape}, Y shape: {Y.shape}")

        # G√©n√©rer pr√©dictions
        preds = load_model_and_predict(
            models_paths[indicator],
            X,
            device
        )
        predictions[indicator] = preds  # (n, 2)

        # Sauvegarder Y_meta (Direction, colonne 0)
        if Y_meta is None:
            Y_meta = Y[:, 0:1]  # (n, 1) - Direction uniquement

    # V√©rifier coh√©rence tailles
    n_samples = Y_meta.shape[0]
    for indicator, preds in predictions.items():
        if preds.shape[0] != n_samples:
            raise ValueError(
                f"{indicator} predictions shape mismatch: "
                f"{preds.shape[0]} vs {n_samples} expected"
            )

    # Concat√©ner pr√©dictions (6 colonnes)
    X_meta = np.concatenate([
        predictions['macd'],  # (n, 2) - [dir, force]
        predictions['rsi'],   # (n, 2)
        predictions['cci'],   # (n, 2)
    ], axis=1)  # (n, 6)

    logger.info(f"\n‚úÖ M√©ta-Features g√©n√©r√©es:")
    logger.info(f"   X_meta shape: {X_meta.shape}")
    logger.info(f"   Y_meta shape: {Y_meta.shape}")
    logger.info(f"   Distribution Direction:")
    logger.info(f"     UP (1):   {np.sum(Y_meta == 1)} ({np.mean(Y_meta)*100:.1f}%)")
    logger.info(f"     DOWN (0): {np.sum(Y_meta == 0)} ({(1-np.mean(Y_meta))*100:.1f}%)")

    return X_meta, Y_meta


def main():
    parser = argparse.ArgumentParser(
        description="G√©n√®re les m√©ta-features pour Stacking"
    )
    parser.add_argument(
        '--assets',
        nargs='+',
        default=['BTC', 'ETH', 'BNB', 'ADA', 'LTC'],
        help='Assets utilis√©s (pour nom de fichier)'
    )
    parser.add_argument(
        '--output-dir',
        default='data/meta',
        help='R√©pertoire de sortie'
    )
    parser.add_argument(
        '--device',
        choices=['auto', 'cuda', 'cpu'],
        default='auto',
        help='Device √† utiliser'
    )

    args = parser.parse_args()

    # Device
    if args.device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    else:
        device = args.device

    logger.info("="*80)
    logger.info("G√âN√âRATION M√âTA-FEATURES POUR STACKING")
    logger.info("="*80)
    logger.info(f"\nDevice: {device}")
    logger.info(f"Assets: {', '.join(args.assets)}")

    # Construire noms de fichiers
    assets_str = '_'.join(args.assets).lower()

    # Chemins des mod√®les
    models_paths = {
        'macd': f'models/best_model_macd_kalman_dual_binary.pth',
        'rsi': f'models/best_model_rsi_kalman_dual_binary.pth',
        'cci': f'models/best_model_cci_kalman_dual_binary.pth',
    }

    # Chemins des datasets
    datasets_paths = {
        'macd': f'data/prepared/dataset_{assets_str}_macd_dual_binary_kalman.npz',
        'rsi': f'data/prepared/dataset_{assets_str}_rsi_dual_binary_kalman.npz',
        'cci': f'data/prepared/dataset_{assets_str}_cci_dual_binary_kalman.npz',
    }

    # V√©rifier existence
    logger.info("\nüìÅ V√©rification fichiers...")
    for indicator, path in models_paths.items():
        if not Path(path).exists():
            logger.error(f"‚ùå Mod√®le manquant: {path}")
            logger.error(f"   Entra√Ænez d'abord le mod√®le {indicator.upper()}:")
            logger.error(f"   python src/train.py --data {datasets_paths[indicator]} --epochs 50")
            return 1

    for indicator, path in datasets_paths.items():
        if not Path(path).exists():
            logger.error(f"‚ùå Dataset manquant: {path}")
            logger.error(f"   G√©n√©rez d'abord les datasets:")
            logger.error(f"   python src/prepare_data_purified_dual_binary.py --assets {' '.join(args.assets)}")
            return 1

    logger.info("‚úÖ Tous les fichiers requis sont pr√©sents")

    # G√©n√©rer m√©ta-features pour chaque split
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    for split_name in ['train', 'val', 'test']:
        X_meta, Y_meta = generate_meta_features_for_split(
            split_name,
            models_paths,
            datasets_paths,
            device
        )

        # Sauvegarder
        output_path = output_dir / f'meta_features_{split_name}.npz'
        np.savez_compressed(
            output_path,
            X_meta=X_meta,
            Y_meta=Y_meta
        )
        logger.info(f"\nüíæ Sauvegard√©: {output_path}")

    logger.info("\n" + "="*80)
    logger.info("‚úÖ TERMIN√â - M√©ta-Features g√©n√©r√©es pour Train/Val/Test")
    logger.info("="*80)
    logger.info("\nProchaine √©tape:")
    logger.info("  python src/train_stacking.py")

    return 0


if __name__ == '__main__':
    import sys
    sys.exit(main())
