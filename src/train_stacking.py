#!/usr/bin/env python3
"""
Entra√Ænement du Meta-Mod√®le pour Stacking / Ensemble Learning

Objectif: Combiner les pr√©dictions de 3 mod√®les experts (MACD, RSI, CCI)
pour am√©liorer la pr√©diction de Direction (Kalman original).

Cible: Direction Kalman (label original, pas de relabeling)

Hypoth√®se: Si le Stacking am√©liore l'Accuracy Direction (92% ‚Üí 95-96%),
la rentabilit√© devrait suivre naturellement car on colle mieux au Kalman.

DONN√âES D'ENTR√âE:
    Les .npz doivent contenir Y_pred (pr√©dictions des mod√®les):
    - dataset_btc_eth_bnb_ada_ltc_macd_dual_binary_kalman.npz (Y_train_pred, Y_val_pred, Y_test_pred)
    - dataset_btc_eth_bnb_ada_ltc_rsi_dual_binary_kalman.npz (Y_train_pred, Y_val_pred, Y_test_pred)
    - dataset_btc_eth_bnb_ada_ltc_cci_dual_binary_kalman.npz (Y_train_pred, Y_val_pred, Y_test_pred)

    Si Y_pred manquant ‚Üí Ex√©cuter: python src/evaluate.py --data <dataset>

Mod√®les test√©s:
  1. Logistic Regression (baseline - RECOMMAND√â)
  2. Random Forest (si non-lin√©aire)
  3. MLP (si tr√®s non-lin√©aire)

Usage:
  python src/train_stacking.py --model logistic
  python src/train_stacking.py --model rf
  python src/train_stacking.py --model mlp
"""

import sys
import numpy as np
from pathlib import Path
import logging
import argparse
from typing import Dict, Tuple

# ML models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)


# =============================================================================
# CHARGEMENT DONN√âES
# =============================================================================

DATASET_PATHS = {
    'macd': 'data/prepared/dataset_btc_eth_bnb_ada_ltc_macd_dual_binary_kalman.npz',
    'rsi': 'data/prepared/dataset_btc_eth_bnb_ada_ltc_rsi_dual_binary_kalman.npz',
    'cci': 'data/prepared/dataset_btc_eth_bnb_ada_ltc_cci_dual_binary_kalman.npz',
}


def load_predictions_from_npz(split: str = 'train') -> Tuple[np.ndarray, np.ndarray]:
    """
    Charge les pr√©dictions des 3 mod√®les depuis les .npz.

    Args:
        split: 'train', 'val', ou 'test'

    Returns:
        X_meta: (n, 6) - Pr√©dictions des 3 mod√®les [macd_dir, macd_force, rsi_dir, rsi_force, cci_dir, cci_force]
        Y_meta: (n, 1) - Direction Kalman (cible commune)
    """
    logger.info(f"\nüìÇ Chargement pr√©dictions split '{split}'...")

    predictions = {}
    Y_meta = None

    for indicator in ['macd', 'rsi', 'cci']:
        path = DATASET_PATHS[indicator]

        if not Path(path).exists():
            raise FileNotFoundError(
                f"‚ùå Dataset introuvable: {path}\n"
                f"   Ex√©cuter: python src/prepare_data_purified_dual_binary.py --assets BTC ETH BNB ADA LTC"
            )

        logger.info(f"   {indicator.upper()}...")
        data = np.load(path, allow_pickle=True)

        # V√©rifier que Y_pred existe
        y_pred_key = f'Y_{split}_pred'
        if y_pred_key not in data:
            raise ValueError(
                f"‚ùå Pr√©dictions manquantes dans {path}\n"
                f"   Cl√© manquante: {y_pred_key}\n"
                f"   Ex√©cuter: python src/evaluate.py --data {path}"
            )

        Y_pred = data[y_pred_key]  # Shape: (n, 2) - [direction, force]
        Y = data[f'Y_{split}']     # Shape: (n, 2) - [direction, force]

        logger.info(f"      Y_pred shape: {Y_pred.shape}")

        predictions[indicator] = Y_pred

        # Utiliser Y du premier indicateur comme cible (tous identiques)
        if Y_meta is None:
            Y_meta = Y[:, 0:1]  # Direction uniquement (shape: n, 1)

    # Concat√©ner pr√©dictions (6 features)
    X_meta = np.concatenate([
        predictions['macd'],  # (n, 2)
        predictions['rsi'],   # (n, 2)
        predictions['cci'],   # (n, 2)
    ], axis=1)  # (n, 6)

    logger.info(f"\n‚úÖ M√©ta-features cr√©√©es:")
    logger.info(f"   X_meta shape: {X_meta.shape}")
    logger.info(f"   Y_meta shape: {Y_meta.shape}")
    logger.info(f"   Features: [MACD_dir, MACD_force, RSI_dir, RSI_force, CCI_dir, CCI_force]")
    logger.info(f"   Cible: Direction Kalman")

    return X_meta, Y_meta


# =============================================================================
# M√âTRIQUES
# =============================================================================

def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
    """Calcule les m√©triques de classification."""
    acc = accuracy_score(y_true, y_pred)
    prec, rec, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average='binary', zero_division=0
    )
    cm = confusion_matrix(y_true, y_pred)

    return {
        'accuracy': acc,
        'precision': prec,
        'recall': rec,
        'f1': f1,
        'confusion_matrix': cm.tolist()
    }


# =============================================================================
# MOD√àLES
# =============================================================================

def train_logistic_regression(
    X_train: np.ndarray,
    Y_train: np.ndarray,
    X_val: np.ndarray,
    Y_val: np.ndarray,
    X_test: np.ndarray,
    Y_test: np.ndarray
) -> Tuple[LogisticRegression, Dict, Dict, Dict]:
    """Entra√Æne une R√©gression Logistique."""
    logger.info("\n" + "="*80)
    logger.info("üéØ Mod√®le: Logistic Regression (Baseline)")
    logger.info("="*80)

    model = LogisticRegression(
        max_iter=1000,
        random_state=42,
        solver='lbfgs'
    )

    logger.info("\n‚è≥ Entra√Ænement...")
    model.fit(X_train, Y_train.ravel())

    # √âvaluation
    y_train_pred = model.predict(X_train)
    y_val_pred = model.predict(X_val)
    y_test_pred = model.predict(X_test)

    metrics_train = compute_metrics(Y_train.ravel(), y_train_pred)
    metrics_val = compute_metrics(Y_val.ravel(), y_val_pred)
    metrics_test = compute_metrics(Y_test.ravel(), y_test_pred)

    logger.info(f"\nüìä R√©sultats:")
    logger.info(f"   Train Accuracy: {metrics_train['accuracy']*100:.2f}%")
    logger.info(f"   Val Accuracy:   {metrics_val['accuracy']*100:.2f}%")
    logger.info(f"   Test Accuracy:  {metrics_test['accuracy']*100:.2f}%")

    gap_train_val = abs(metrics_train['accuracy'] - metrics_val['accuracy']) * 100
    gap_val_test = abs(metrics_val['accuracy'] - metrics_test['accuracy']) * 100
    logger.info(f"\n   Gap Train/Val: {gap_train_val:.2f}%")
    logger.info(f"   Gap Val/Test:  {gap_val_test:.2f}%")

    # Poids des features (interpr√©tabilit√©)
    logger.info(f"\nüìà Poids des features (interpr√©tabilit√©):")
    feature_names = ['MACD_dir', 'MACD_force', 'RSI_dir', 'RSI_force', 'CCI_dir', 'CCI_force']
    for name, weight in zip(feature_names, model.coef_[0]):
        logger.info(f"     {name:12s}: {weight:+.4f}")

    return model, metrics_train, metrics_val, metrics_test


def train_random_forest(
    X_train: np.ndarray,
    Y_train: np.ndarray,
    X_val: np.ndarray,
    Y_val: np.ndarray,
    X_test: np.ndarray,
    Y_test: np.ndarray
) -> Tuple[RandomForestClassifier, Dict, Dict, Dict]:
    """Entra√Æne un Random Forest."""
    logger.info("\n" + "="*80)
    logger.info("üå≤ Mod√®le: Random Forest (Non-Lin√©aire)")
    logger.info("="*80)

    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        n_jobs=-1
    )

    logger.info("\n‚è≥ Entra√Ænement...")
    model.fit(X_train, Y_train.ravel())

    # √âvaluation
    y_train_pred = model.predict(X_train)
    y_val_pred = model.predict(X_val)
    y_test_pred = model.predict(X_test)

    metrics_train = compute_metrics(Y_train.ravel(), y_train_pred)
    metrics_val = compute_metrics(Y_val.ravel(), y_val_pred)
    metrics_test = compute_metrics(Y_test.ravel(), y_test_pred)

    logger.info(f"\nüìä R√©sultats:")
    logger.info(f"   Train Accuracy: {metrics_train['accuracy']*100:.2f}%")
    logger.info(f"   Val Accuracy:   {metrics_val['accuracy']*100:.2f}%")
    logger.info(f"   Test Accuracy:  {metrics_test['accuracy']*100:.2f}%")

    gap_train_val = abs(metrics_train['accuracy'] - metrics_val['accuracy']) * 100
    gap_val_test = abs(metrics_val['accuracy'] - metrics_test['accuracy']) * 100
    logger.info(f"\n   Gap Train/Val: {gap_train_val:.2f}%")
    logger.info(f"   Gap Val/Test:  {gap_val_test:.2f}%")

    # Feature importance
    logger.info(f"\nüìà Feature Importance:")
    feature_names = ['MACD_dir', 'MACD_force', 'RSI_dir', 'RSI_force', 'CCI_dir', 'CCI_force']
    importances = sorted(zip(feature_names, model.feature_importances_),
                        key=lambda x: x[1], reverse=True)
    for name, importance in importances:
        logger.info(f"     {name:12s}: {importance:.4f}")

    return model, metrics_train, metrics_val, metrics_test


class SimpleMLP(nn.Module):
    """MLP simple pour m√©ta-apprentissage."""
    def __init__(self, input_size=6, hidden_size=32, dropout=0.3):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, 16),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.layers(x)


def train_mlp(
    X_train: np.ndarray,
    Y_train: np.ndarray,
    X_val: np.ndarray,
    Y_val: np.ndarray,
    X_test: np.ndarray,
    Y_test: np.ndarray,
    device: str = 'cpu',
    epochs: int = 50,
    batch_size: int = 128,
    lr: float = 0.001
) -> Tuple[SimpleMLP, Dict, Dict, Dict]:
    """Entra√Æne un MLP."""
    logger.info("\n" + "="*80)
    logger.info("üß† Mod√®le: MLP (Deep Learning)")
    logger.info("="*80)

    # Pr√©paration donn√©es
    X_train_t = torch.FloatTensor(X_train).to(device)
    Y_train_t = torch.FloatTensor(Y_train).to(device)
    X_val_t = torch.FloatTensor(X_val).to(device)
    Y_val_t = torch.FloatTensor(Y_val).to(device)
    X_test_t = torch.FloatTensor(X_test).to(device)
    Y_test_t = torch.FloatTensor(Y_test).to(device)

    train_dataset = TensorDataset(X_train_t, Y_train_t)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # Mod√®le
    model = SimpleMLP(input_size=6, hidden_size=32, dropout=0.3).to(device)
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    logger.info(f"\n‚è≥ Entra√Ænement ({epochs} √©poques)...")

    best_val_acc = 0
    patience_counter = 0
    patience = 10

    for epoch in range(epochs):
        model.train()
        train_loss = 0

        for X_batch, Y_batch in train_loader:
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, Y_batch)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        # √âvaluation
        model.eval()
        with torch.no_grad():
            y_val_pred = (model(X_val_t) > 0.5).float().cpu().numpy()
            val_acc = accuracy_score(Y_val.ravel(), y_val_pred.ravel())

        if (epoch + 1) % 10 == 0:
            logger.info(f"   √âpoque {epoch+1:3d}: Val Acc = {val_acc*100:.2f}%")

        # Early stopping
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                logger.info(f"   Early stopping √† l'√©poque {epoch+1}")
                break

    # √âvaluation finale
    model.eval()
    with torch.no_grad():
        y_train_pred = (model(X_train_t) > 0.5).float().cpu().numpy()
        y_val_pred = (model(X_val_t) > 0.5).float().cpu().numpy()
        y_test_pred = (model(X_test_t) > 0.5).float().cpu().numpy()

    metrics_train = compute_metrics(Y_train.ravel(), y_train_pred.ravel())
    metrics_val = compute_metrics(Y_val.ravel(), y_val_pred.ravel())
    metrics_test = compute_metrics(Y_test.ravel(), y_test_pred.ravel())

    logger.info(f"\nüìä R√©sultats:")
    logger.info(f"   Train Accuracy: {metrics_train['accuracy']*100:.2f}%")
    logger.info(f"   Val Accuracy:   {metrics_val['accuracy']*100:.2f}%")
    logger.info(f"   Test Accuracy:  {metrics_test['accuracy']*100:.2f}%")

    gap_train_val = abs(metrics_train['accuracy'] - metrics_val['accuracy']) * 100
    gap_val_test = abs(metrics_val['accuracy'] - metrics_test['accuracy']) * 100
    logger.info(f"\n   Gap Train/Val: {gap_train_val:.2f}%")
    logger.info(f"   Gap Val/Test:  {gap_val_test:.2f}%")

    return model, metrics_train, metrics_val, metrics_test


# =============================================================================
# MAIN
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description='Stacking - Entra√Ænement Meta-Mod√®le')
    parser.add_argument(
        '--model',
        type=str,
        choices=['logistic', 'rf', 'mlp'],
        default='logistic',
        help="Mod√®le √† entra√Æner (d√©faut: logistic)"
    )
    parser.add_argument(
        '--device',
        type=str,
        default='cpu',
        choices=['cpu', 'cuda'],
        help="Device pour MLP (d√©faut: cpu)"
    )

    args = parser.parse_args()

    logger.info("="*80)
    logger.info("ü§ñ STACKING - Entra√Ænement Meta-Mod√®le")
    logger.info("="*80)
    logger.info(f"\nüéØ Objectif: Combiner MACD, RSI, CCI pour am√©liorer Direction")
    logger.info(f"üìä Cible: Direction Kalman (original, pas de relabeling)")
    logger.info(f"üìà Attendu: Accuracy 92% ‚Üí 95-96%, Win Rate 14% ‚Üí 55-65%")

    # Charger donn√©es
    logger.info("\n" + "="*80)
    logger.info("üìÇ CHARGEMENT DONN√âES")
    logger.info("="*80)

    X_train, Y_train = load_predictions_from_npz('train')
    X_val, Y_val = load_predictions_from_npz('val')
    X_test, Y_test = load_predictions_from_npz('test')

    # Entra√Æner mod√®le
    logger.info("\n" + "="*80)
    logger.info("‚è≥ ENTRA√éNEMENT")
    logger.info("="*80)

    if args.model == 'logistic':
        model, metrics_train, metrics_val, metrics_test = train_logistic_regression(
            X_train, Y_train, X_val, Y_val, X_test, Y_test
        )
    elif args.model == 'rf':
        model, metrics_train, metrics_val, metrics_test = train_random_forest(
            X_train, Y_train, X_val, Y_val, X_test, Y_test
        )
    elif args.model == 'mlp':
        model, metrics_train, metrics_val, metrics_test = train_mlp(
            X_train, Y_train, X_val, Y_val, X_test, Y_test,
            device=args.device
        )

    # Crit√®res de succ√®s
    logger.info("\n" + "="*80)
    logger.info("‚úÖ CRIT√àRES DE SUCC√àS")
    logger.info("="*80)

    test_acc = metrics_test['accuracy'] * 100
    gap_train_test = abs(metrics_train['accuracy'] - metrics_test['accuracy']) * 100

    success_criteria = {
        'Test Accuracy ‚â• 95%': test_acc >= 95,
        'Gap Train/Test < 5%': gap_train_test < 5,
    }

    for criterion, passed in success_criteria.items():
        status = "‚úÖ" if passed else "‚ùå"
        logger.info(f"   {status} {criterion}")

    all_passed = all(success_criteria.values())

    if all_passed:
        logger.info(f"\nüèÜ SUCC√àS! Tous les crit√®res pass√©s!")
        logger.info(f"   ‚Üí Prochaine √©tape: Backtest pour v√©rifier Win Rate > 50%")
    else:
        logger.info(f"\n‚ö†Ô∏è  Crit√®res non atteints. Diagnostiquer:")
        logger.info(f"   - V√©rifier diversit√© des 3 mod√®les de base")
        logger.info(f"   - Tester avec d'autres features (volatilit√©, volume)")

    logger.info("\n" + "="*80)
    logger.info("üèÅ FIN")
    logger.info("="*80)


if __name__ == '__main__':
    sys.exit(main())
