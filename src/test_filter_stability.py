"""
Test de stabilit√© des filtres sur fen√™tre glissante.

Objectif:
    Tester si un filtre produit le m√™me label (pente) qu'on l'applique sur
    une fen√™tre locale (ex: t-100 √† t) ou sur tout le dataset.

Protocole:
    1. Appliquer filtre sur TOUT le dataset ‚Üí labels globaux
    2. Pour 200 positions √©chantillonn√©es:
       - Appliquer filtre sur fen√™tre [t-100:t] ‚Üí label local
       - Comparer label local vs label global
    3. Calculer concordance (% d'accord)

Usage:
    python src/test_filter_stability.py \
        --data data_trad/BTCUSD_all_5m.csv \
        --filter octave \
        --window-size 100 \
        --n-samples 200
"""

import numpy as np
import pandas as pd
import argparse
from pathlib import Path
import sys
from typing import Tuple, Dict

# Ajouter src au path
sys.path.insert(0, str(Path(__file__).parent))

from filters import signal_filtfilt


def load_data(file_path: str) -> pd.DataFrame:
    """Charge les donn√©es CSV."""
    print(f"üìÇ Chargement {file_path}...")
    df = pd.read_csv(file_path)

    # V√©rifier colonnes requises
    if 'close' not in df.columns:
        raise ValueError("Colonne 'close' manquante dans le CSV")

    print(f"   Samples: {len(df):,}")
    print(f"   Colonnes: {list(df.columns)}")

    return df


def apply_filter(data: np.ndarray, filter_name: str, **kwargs) -> np.ndarray:
    """Applique un filtre sur les donn√©es."""
    if filter_name == 'octave':
        step = kwargs.get('step', 0.20)  # Octave20 = step 0.20
        order = kwargs.get('order', 3)
        return signal_filtfilt(data, step=step, order=order)
    else:
        raise ValueError(f"Filtre inconnu: {filter_name}")


def compute_slope_label(filtered: np.ndarray, index: int) -> int:
    """
    Calcule le label de pente √† l'index donn√©.

    Label = 1 si filtered[index-2] > filtered[index-3], sinon 0
    """
    if index < 3:
        return -1  # Pas assez de donn√©es

    return int(filtered[index - 2] > filtered[index - 3])


def test_filter_stability(
    data: np.ndarray,
    filter_name: str,
    window_size: int = 100,
    n_samples: int = 200,
    **filter_kwargs
) -> Dict[str, float]:
    """
    Teste la stabilit√© d'un filtre sur fen√™tre glissante.

    Args:
        data: Donn√©es brutes (Close)
        filter_name: Nom du filtre ('octave', 'savgol', etc.)
        window_size: Taille de la fen√™tre locale
        n_samples: Nombre de positions √† √©chantillonner
        **filter_kwargs: Param√®tres du filtre

    Returns:
        Dict avec statistiques de concordance
    """
    n = len(data)

    print(f"\n{'='*80}")
    print(f"TEST DE STABILIT√â - {filter_name.upper()}")
    print(f"{'='*80}")

    print(f"\n‚öôÔ∏è Configuration:")
    print(f"   Filtre: {filter_name}")
    print(f"   Taille fen√™tre: {window_size}")
    print(f"   Samples test√©s: {n_samples}")
    print(f"   Param√®tres filtre: {filter_kwargs}")

    # √âTAPE 1: Appliquer filtre sur TOUT le dataset (label global)
    print(f"\nüîß √âtape 1: Application du filtre sur TOUT le dataset...")
    filtered_global = apply_filter(data, filter_name, **filter_kwargs)

    # Calculer tous les labels globaux
    labels_global = np.zeros(n, dtype=int)
    for i in range(3, n):
        labels_global[i] = compute_slope_label(filtered_global, i)

    n_up_global = (labels_global == 1).sum()
    n_down_global = (labels_global == 0).sum()
    print(f"   Labels globaux: {n_up_global:,} UP ({n_up_global/n*100:.1f}%), "
          f"{n_down_global:,} DOWN ({n_down_global/n*100:.1f}%)")

    # √âTAPE 2: √âchantillonner les positions √† tester
    # √âviter les bords (besoin de window_size avant et 3 apr√®s pour le label)
    min_idx = window_size + 3
    max_idx = n - 3

    if max_idx <= min_idx:
        raise ValueError(f"Dataset trop petit: {n} samples, besoin de >= {window_size + 6}")

    sample_indices = np.linspace(min_idx, max_idx, n_samples, dtype=int)

    print(f"\nüîß √âtape 2: Test sur fen√™tre glissante ({n_samples} positions)...")
    print(f"   Indices test√©s: [{sample_indices[0]}, ..., {sample_indices[-1]}]")

    # √âTAPE 3: Pour chaque position, appliquer filtre sur fen√™tre locale
    concordance = []
    labels_local = []
    labels_global_sampled = []

    for i, t in enumerate(sample_indices):
        # Fen√™tre locale: [t-window_size : t+1]
        window_start = t - window_size
        window_end = t + 1
        window_data = data[window_start:window_end]

        # Appliquer filtre sur fen√™tre
        filtered_local = apply_filter(window_data, filter_name, **filter_kwargs)

        # Calculer label local
        # Dans la fen√™tre, t correspond √† l'index -1 (dernier √©l√©ment)
        # Donc on compare filtered_local[-2] vs filtered_local[-3]
        label_local = int(filtered_local[-2] > filtered_local[-3])

        # Label global √† cette position
        label_global = labels_global[t]

        # Concordance
        agree = (label_local == label_global)
        concordance.append(agree)
        labels_local.append(label_local)
        labels_global_sampled.append(label_global)

        # Affichage progressif
        if (i + 1) % 50 == 0:
            current_concordance = np.mean(concordance) * 100
            print(f"   Progression: {i+1}/{n_samples} - Concordance actuelle: {current_concordance:.1f}%")

    # √âTAPE 4: Statistiques finales
    concordance = np.array(concordance)
    labels_local = np.array(labels_local)
    labels_global_sampled = np.array(labels_global_sampled)

    concordance_pct = concordance.mean() * 100

    # Concordance par classe
    mask_up_global = (labels_global_sampled == 1)
    mask_down_global = (labels_global_sampled == 0)

    concordance_up = concordance[mask_up_global].mean() * 100 if mask_up_global.any() else 0
    concordance_down = concordance[mask_down_global].mean() * 100 if mask_down_global.any() else 0

    # Distribution des labels locaux
    n_up_local = (labels_local == 1).sum()
    n_down_local = (labels_local == 0).sum()

    print(f"\n{'='*80}")
    print("R√âSULTATS")
    print(f"{'='*80}")

    print(f"\nüìä Labels locaux vs globaux:")
    print(f"   Global √©chantillonn√©: {mask_up_global.sum()} UP, {mask_down_global.sum()} DOWN")
    print(f"   Local (fen√™tre):      {n_up_local} UP, {n_down_local} DOWN")

    print(f"\n‚úÖ Concordance globale: {concordance_pct:.2f}%")
    print(f"   - Sur labels UP:   {concordance_up:.2f}%")
    print(f"   - Sur labels DOWN: {concordance_down:.2f}%")

    # Analyse des d√©saccords
    n_disagree = (~concordance).sum()
    print(f"\n‚ùå D√©saccords: {n_disagree}/{n_samples} ({n_disagree/n_samples*100:.1f}%)")

    if n_disagree > 0:
        # Types de d√©saccords
        flip_to_up = ((labels_global_sampled == 0) & (labels_local == 1)).sum()
        flip_to_down = ((labels_global_sampled == 1) & (labels_local == 0)).sum()
        print(f"   - Global DOWN ‚Üí Local UP:   {flip_to_up}")
        print(f"   - Global UP   ‚Üí Local DOWN: {flip_to_down}")

    return {
        'concordance': concordance_pct,
        'concordance_up': concordance_up,
        'concordance_down': concordance_down,
        'n_samples': n_samples,
        'n_disagree': n_disagree
    }


def main():
    parser = argparse.ArgumentParser(
        description="Test de stabilit√© des filtres sur fen√™tre glissante",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # Donn√©es
    parser.add_argument('--data', type=str, required=True,
                        help='Fichier CSV avec les donn√©es (doit contenir colonne "close")')

    # Filtre
    parser.add_argument('--filter', type=str, default='octave',
                        choices=['octave'],
                        help='Filtre √† tester')
    parser.add_argument('--step', type=float, default=0.20,
                        help='Param√®tre step pour Octave (0.20 = Octave20)')
    parser.add_argument('--order', type=int, default=3,
                        help='Ordre du filtre Butterworth')

    # Test
    parser.add_argument('--window-size', type=int, default=100,
                        help='Taille de la fen√™tre locale')
    parser.add_argument('--n-samples', type=int, default=200,
                        help='Nombre de positions √† √©chantillonner')

    args = parser.parse_args()

    print("="*80)
    print("TEST DE STABILIT√â DES FILTRES")
    print("="*80)

    # Charger donn√©es
    df = load_data(args.data)
    close = df['close'].values

    print(f"\nüìä Statistiques des donn√©es:")
    print(f"   Close: min={close.min():.2f}, max={close.max():.2f}, "
          f"mean={close.mean():.2f}, std={close.std():.2f}")

    # Param√®tres du filtre
    filter_kwargs = {}
    if args.filter == 'octave':
        filter_kwargs = {'step': args.step, 'order': args.order}

    # Test de stabilit√©
    results = test_filter_stability(
        data=close,
        filter_name=args.filter,
        window_size=args.window_size,
        n_samples=args.n_samples,
        **filter_kwargs
    )

    # R√©sum√© final
    print(f"\n{'='*80}")
    print("R√âSUM√â")
    print(f"{'='*80}")

    print(f"\nüéØ Filtre test√©: {args.filter.upper()}")
    print(f"   Param√®tres: {filter_kwargs}")
    print(f"   Fen√™tre: {args.window_size} samples")

    print(f"\nüìà Concordance: {results['concordance']:.2f}%")

    if results['concordance'] >= 95:
        print("\n‚úÖ EXCELLENT - Le filtre est tr√®s stable sur fen√™tre glissante")
    elif results['concordance'] >= 85:
        print("\n‚úÖ BON - Le filtre est stable")
    elif results['concordance'] >= 70:
        print("\n‚ö†Ô∏è MOYEN - Le filtre a une stabilit√© acceptable")
    else:
        print("\n‚ùå FAIBLE - Le filtre est instable sur fen√™tre glissante")

    print("\n" + "="*80)


if __name__ == '__main__':
    main()
