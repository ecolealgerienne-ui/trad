"""
Script de pr√©paration des donn√©es.

Pr√©pare les datasets (X, Y) et les sauvegarde en format numpy (.npz).
Permet de gagner du temps en √©vitant de recalculer les donn√©es √† chaque entra√Ænement.

Usage:
    python src/prepare_data.py --timeframe 5 --filter kalman
    python src/prepare_data.py --timeframe 1 --filter decycler
    python src/prepare_data.py --timeframe all --filter kalman  # Combine 1min + 5min!
"""

import numpy as np
import argparse
import logging
from pathlib import Path
import json
from datetime import datetime

logger = logging.getLogger(__name__)

# Import modules locaux
from constants import (
    BTC_DATA_FILE_1M, ETH_DATA_FILE_1M,
    BTC_DATA_FILE_5M, ETH_DATA_FILE_5M,
    BTC_CANDLES, ETH_CANDLES,
    TRIM_EDGES,
    TRAIN_SPLIT, VAL_SPLIT, TEST_SPLIT,
    PREPARED_DATA_DIR, PREPARED_DATA_FILE,
    LABEL_FILTER_TYPE,
    SEQUENCE_LENGTH, NUM_INDICATORS,
    RSI_PERIOD, CCI_PERIOD, MACD_FAST, MACD_SLOW, MACD_SIGNAL,
    BOL_PERIOD, BOL_NUM_STD,
    DECYCLER_CUTOFF, KALMAN_PROCESS_VAR, KALMAN_MEASURE_VAR
)
from data_utils import load_crypto_data, trim_edges
from indicators import (
    prepare_datasets,
    calculate_all_indicators_for_model,
    generate_all_labels,
    create_sequences
)


def prepare_single_asset(df, filter_type: str, asset_name: str = "Asset") -> tuple:
    """
    Calcule indicateurs + labels + s√©quences pour UN SEUL asset.

    IMPORTANT: Les indicateurs doivent √™tre calcul√©s S√âPAR√âMENT par asset
    avant de merger, sinon les valeurs de fin d'un asset polluent le d√©but
    du suivant (RSI, CCI, MACD utilisent les N valeurs pr√©c√©dentes).

    Args:
        df: DataFrame avec OHLC pour un seul asset
        filter_type: 'kalman' ou 'decycler'
        asset_name: Nom pour les logs

    Returns:
        (X, Y) o√π X shape=(n_sequences, 12, 4), Y shape=(n_sequences, 4)
    """
    logger.info(f"  üìà {asset_name}: Calcul indicateurs ({len(df):,} bougies)...")

    # 1. Calculer indicateurs (RSI, CCI, BOL, MACD)
    indicators = calculate_all_indicators_for_model(df)

    # 2. G√©n√©rer labels avec filtre
    labels = generate_all_labels(indicators, filter_type=filter_type)

    # 3. Cr√©er s√©quences de 12 timesteps
    X, Y = create_sequences(indicators, labels)

    logger.info(f"     ‚Üí X={X.shape}, Y={Y.shape}")

    return X, Y


def split_sequences(X, Y, train_ratio=TRAIN_SPLIT, val_ratio=VAL_SPLIT,
                    test_ratio=TEST_SPLIT, random_seed=42):
    """
    Split les s√©quences (pas les donn√©es brutes) avec:
    - TEST = fin (donn√©es les plus r√©centes)
    - VAL = √©chantillonn√© al√©atoirement du reste
    - TRAIN = le reste

    IMPORTANT: Le split est fait sur les S√âQUENCES d√©j√† cr√©√©es, pas sur les
    donn√©es brutes. Cela garantit que les indicateurs ont √©t√© calcul√©s sur
    des donn√©es contigu√´s.

    Args:
        X: array de shape (n_sequences, seq_len, n_features)
        Y: array de shape (n_sequences, n_outputs)
        train_ratio, val_ratio, test_ratio: proportions
        random_seed: seed pour reproductibilit√©

    Returns:
        (X_train, Y_train), (X_val, Y_val), (X_test, Y_test)
    """
    # V√©rifier les ratios
    total_ratio = train_ratio + val_ratio + test_ratio
    if abs(total_ratio - 1.0) > 0.01:
        raise ValueError(f"Ratios doivent sommer √† 1.0, got {total_ratio}")

    n_total = len(X)
    n_test = int(n_total * test_ratio)
    n_val = int(n_total * val_ratio)

    # 1. TEST = toujours √† la fin (donn√©es les plus r√©centes)
    X_test = X[-n_test:]
    Y_test = Y[-n_test:]

    # Remaining (pour train + val)
    X_remaining = X[:-n_test]
    Y_remaining = Y[:-n_test]

    # 2. VAL = √©chantillonn√© al√©atoirement du reste
    np.random.seed(random_seed)
    val_indices = np.random.choice(len(X_remaining), size=n_val, replace=False)
    train_indices = np.setdiff1d(np.arange(len(X_remaining)), val_indices)

    X_val = X_remaining[val_indices]
    Y_val = Y_remaining[val_indices]

    # 3. TRAIN = le reste
    X_train = X_remaining[train_indices]
    Y_train = Y_remaining[train_indices]

    # V√©rification finale
    assert len(X_train) + len(X_val) + len(X_test) == n_total, \
        f"Split error: {len(X_train)} + {len(X_val)} + {len(X_test)} != {n_total}"

    return (X_train, Y_train), (X_val, Y_val), (X_test, Y_test)


def prepare_and_save(timeframe: str = '5',
                     filter_type: str = LABEL_FILTER_TYPE,
                     output_path: str = None,
                     btc_candles: int = None,
                     eth_candles: int = None) -> str:
    """
    Pr√©pare les donn√©es et les sauvegarde en format numpy.

    Args:
        timeframe: '1', '5', ou 'all' (combine 1min + 5min)
        filter_type: 'decycler' ou 'kalman'
        output_path: Chemin de sortie (d√©faut: auto-g√©n√©r√©)
        btc_candles: Nombre de bougies BTC par timeframe (d√©faut: toutes)
        eth_candles: Nombre de bougies ETH par timeframe (d√©faut: toutes)

    Returns:
        Chemin du fichier sauvegard√©
    """
    logger.info("="*80)
    logger.info("PR√âPARATION DES DONN√âES")
    logger.info("="*80)

    timeframe = str(timeframe)  # Convertir en string
    total_btc = 0
    total_eth = 0

    # Charger selon le timeframe
    if timeframe in ['1', '5']:
        # Un seul timeframe
        if timeframe == '1':
            btc_file = BTC_DATA_FILE_1M
            eth_file = ETH_DATA_FILE_1M
            logger.info(f"üìä Timeframe: 1 minute")
        else:
            btc_file = BTC_DATA_FILE_5M
            eth_file = ETH_DATA_FILE_5M
            logger.info(f"üìä Timeframe: 5 minutes")

        logger.info(f"   ‚Üí Indicateurs calcul√©s PAR ASSET sur donn√©es CONTIGU√ãS")
        logger.info(f"   ‚Üí Split appliqu√© aux S√âQUENCES (pas aux donn√©es brutes)")

        btc = load_crypto_data(btc_file, n_candles=btc_candles, asset_name='BTC')
        eth = load_crypto_data(eth_file, n_candles=eth_candles, asset_name='ETH')

        btc_trimmed = trim_edges(btc, trim_start=TRIM_EDGES, trim_end=TRIM_EDGES)
        eth_trimmed = trim_edges(eth, trim_start=TRIM_EDGES, trim_end=TRIM_EDGES)

        total_btc = len(btc)
        total_eth = len(eth)

        logger.info(f"üîß Filtre pour labels: {filter_type}")

        # =====================================================================
        # CALCUL INDICATEURS SUR DONN√âES CONTIGU√ãS (AVANT SPLIT!)
        # =====================================================================
        logger.info(f"\nüìà Calcul des indicateurs sur donn√©es CONTIGU√ãS...")

        X_btc, Y_btc = prepare_single_asset(btc_trimmed, filter_type, "BTC")
        X_eth, Y_eth = prepare_single_asset(eth_trimmed, filter_type, "ETH")

        # =====================================================================
        # SPLIT DES S√âQUENCES (Test=fin, Val=√©chantillonn√©)
        # =====================================================================
        logger.info(f"\nüìä Split des s√©quences (Test=fin, Val=√©chantillonn√©)...")

        (X_btc_train, Y_btc_train), (X_btc_val, Y_btc_val), (X_btc_test, Y_btc_test) = \
            split_sequences(X_btc, Y_btc)
        (X_eth_train, Y_eth_train), (X_eth_val, Y_eth_val), (X_eth_test, Y_eth_test) = \
            split_sequences(X_eth, Y_eth)

        logger.info(f"  BTC: Train={len(X_btc_train)}, Val={len(X_btc_val)}, Test={len(X_btc_test)}")
        logger.info(f"  ETH: Train={len(X_eth_train)}, Val={len(X_eth_val)}, Test={len(X_eth_test)}")

        # Concat√©ner les assets
        X_train = np.concatenate([X_btc_train, X_eth_train], axis=0)
        Y_train = np.concatenate([Y_btc_train, Y_eth_train], axis=0)
        X_val = np.concatenate([X_btc_val, X_eth_val], axis=0)
        Y_val = np.concatenate([Y_btc_val, Y_eth_val], axis=0)
        X_test = np.concatenate([X_btc_test, X_eth_test], axis=0)
        Y_test = np.concatenate([Y_btc_test, Y_eth_test], axis=0)

        # Aller directement √† la sauvegarde
        logger.info(f"\nüìä Shapes des datasets:")
        logger.info(f"  Train: X={X_train.shape}, Y={Y_train.shape}")
        logger.info(f"  Val:   X={X_val.shape}, Y={Y_val.shape}")
        logger.info(f"  Test:  X={X_test.shape}, Y={Y_test.shape}")

        # Cr√©er le r√©pertoire de sortie
        if output_path is None:
            output_path = f"data/prepared/dataset_{timeframe}m_{filter_type}.npz"

        output_dir = Path(output_path).parent
        output_dir.mkdir(parents=True, exist_ok=True)

        # M√©tadonn√©es
        metadata = {
            'created_at': datetime.now().isoformat(),
            'timeframe': timeframe,
            'filter_type': filter_type,
            'btc_candles': total_btc,
            'eth_candles': total_eth,
            'total_candles': len(btc_trimmed) + len(eth_trimmed),
            'train_size': len(X_train),
            'val_size': len(X_val),
            'test_size': len(X_test),
            'sequence_length': SEQUENCE_LENGTH,
            'num_indicators': NUM_INDICATORS,
            'indicator_params': {
                'rsi_period': RSI_PERIOD,
                'cci_period': CCI_PERIOD,
                'bol_period': BOL_PERIOD,
                'bol_num_std': BOL_NUM_STD,
                'macd_fast': MACD_FAST,
                'macd_slow': MACD_SLOW,
                'macd_signal': MACD_SIGNAL
            },
            'filter_params': {
                'decycler_cutoff': DECYCLER_CUTOFF,
                'kalman_process_var': KALMAN_PROCESS_VAR,
                'kalman_measure_var': KALMAN_MEASURE_VAR
            },
            'splits': {
                'train': TRAIN_SPLIT,
                'val': VAL_SPLIT,
                'test': TEST_SPLIT
            }
        }

        # Sauvegarder
        np.savez_compressed(
            output_path,
            X_train=X_train,
            Y_train=Y_train,
            X_val=X_val,
            Y_val=Y_val,
            X_test=X_test,
            Y_test=Y_test,
            metadata=json.dumps(metadata)
        )

        logger.info(f"\n‚úÖ Donn√©es sauvegard√©es: {output_path}")
        logger.info(f"   Taille: {Path(output_path).stat().st_size / 1024 / 1024:.1f} MB")

        # Sauvegarder m√©tadonn√©es
        metadata_path = str(output_path).replace('.npz', '_metadata.json')
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        logger.info(f"   M√©tadonn√©es: {metadata_path}")

        return output_path

    elif timeframe == 'all':
        # Train = 1min + 5min, Val/Test = 5min seulement
        logger.info(f"üìä Timeframe: ALL (1min + 5min pour TRAIN, 5min pour VAL/TEST)")
        logger.info(f"   ‚Üí Plus de donn√©es train = meilleure g√©n√©ralisation!")
        logger.info(f"   ‚Üí Val/Test sur 5min = √©valuation r√©aliste!")
        logger.info(f"   ‚Üí Indicateurs calcul√©s PAR ASSET (pas de pollution entre assets)")

        # Charger 1min (pour train)
        logger.info(f"\nüîπ Chargement donn√©es 1 minute (train)...")
        btc_1m = load_crypto_data(BTC_DATA_FILE_1M, n_candles=btc_candles, asset_name='BTC-1m')
        eth_1m = load_crypto_data(ETH_DATA_FILE_1M, n_candles=eth_candles, asset_name='ETH-1m')

        btc_1m_trimmed = trim_edges(btc_1m, trim_start=TRIM_EDGES, trim_end=TRIM_EDGES)
        eth_1m_trimmed = trim_edges(eth_1m, trim_start=TRIM_EDGES, trim_end=TRIM_EDGES)

        # Charger 5min (pour train + val + test)
        logger.info(f"\nüîπ Chargement donn√©es 5 minutes (train + val + test)...")
        btc_5m = load_crypto_data(BTC_DATA_FILE_5M, n_candles=btc_candles, asset_name='BTC-5m')
        eth_5m = load_crypto_data(ETH_DATA_FILE_5M, n_candles=eth_candles, asset_name='ETH-5m')

        btc_5m_trimmed = trim_edges(btc_5m, trim_start=TRIM_EDGES, trim_end=TRIM_EDGES)
        eth_5m_trimmed = trim_edges(eth_5m, trim_start=TRIM_EDGES, trim_end=TRIM_EDGES)

        total_btc = len(btc_1m) + len(btc_5m)
        total_eth = len(eth_1m) + len(eth_5m)

        logger.info(f"\nüìà Donn√©es charg√©es:")
        logger.info(f"   1min: BTC={len(btc_1m_trimmed):,} + ETH={len(eth_1m_trimmed):,}")
        logger.info(f"   5min: BTC={len(btc_5m_trimmed):,} + ETH={len(eth_5m_trimmed):,}")

        logger.info(f"\nüîß Filtre pour labels: {filter_type}")

        # =====================================================================
        # CALCUL INDICATEURS SUR DONN√âES CONTIGU√ãS (AVANT SPLIT!)
        # =====================================================================
        logger.info(f"\nüìà Calcul des indicateurs sur donn√©es CONTIGU√ãS...")

        # 1min - tout pour le train
        logger.info(f"\nüèãÔ∏è Donn√©es 1min (tout pour train):")
        X_btc_1m, Y_btc_1m = prepare_single_asset(btc_1m_trimmed, filter_type, "BTC-1m")
        X_eth_1m, Y_eth_1m = prepare_single_asset(eth_1m_trimmed, filter_type, "ETH-1m")

        # 5min - sera splitt√©
        logger.info(f"\nüìä Donn√©es 5min (sera splitt√©):")
        X_btc_5m, Y_btc_5m = prepare_single_asset(btc_5m_trimmed, filter_type, "BTC-5m")
        X_eth_5m, Y_eth_5m = prepare_single_asset(eth_5m_trimmed, filter_type, "ETH-5m")

        # =====================================================================
        # SPLIT DES S√âQUENCES 5min (Test=fin, Val=√©chantillonn√©)
        # =====================================================================
        logger.info(f"\nüìä Split des s√©quences 5min (Test=fin, Val=√©chantillonn√©)...")

        (X_btc_5m_train, Y_btc_5m_train), (X_btc_5m_val, Y_btc_5m_val), (X_btc_5m_test, Y_btc_5m_test) = \
            split_sequences(X_btc_5m, Y_btc_5m)
        (X_eth_5m_train, Y_eth_5m_train), (X_eth_5m_val, Y_eth_5m_val), (X_eth_5m_test, Y_eth_5m_test) = \
            split_sequences(X_eth_5m, Y_eth_5m)

        logger.info(f"  BTC-5m: Train={len(X_btc_5m_train)}, Val={len(X_btc_5m_val)}, Test={len(X_btc_5m_test)}")
        logger.info(f"  ETH-5m: Train={len(X_eth_5m_train)}, Val={len(X_eth_5m_val)}, Test={len(X_eth_5m_test)}")

        # Combiner: Train = 1min + 5min_train
        X_train = np.concatenate([X_btc_1m, X_eth_1m, X_btc_5m_train, X_eth_5m_train], axis=0)
        Y_train = np.concatenate([Y_btc_1m, Y_eth_1m, Y_btc_5m_train, Y_eth_5m_train], axis=0)

        # Val = 5min_val seulement
        X_val = np.concatenate([X_btc_5m_val, X_eth_5m_val], axis=0)
        Y_val = np.concatenate([Y_btc_5m_val, Y_eth_5m_val], axis=0)

        # Test = 5min_test seulement
        X_test = np.concatenate([X_btc_5m_test, X_eth_5m_test], axis=0)
        Y_test = np.concatenate([Y_btc_5m_test, Y_eth_5m_test], axis=0)

        # Aller directement √† la sauvegarde
        logger.info(f"\nüìä Shapes des datasets:")
        logger.info(f"  Train: X={X_train.shape}, Y={Y_train.shape}")
        logger.info(f"  Val:   X={X_val.shape}, Y={Y_val.shape}")
        logger.info(f"  Test:  X={X_test.shape}, Y={Y_test.shape}")

        # Cr√©er le r√©pertoire de sortie
        if output_path is None:
            output_path = f"data/prepared/dataset_all_{filter_type}.npz"

        output_dir = Path(output_path).parent
        output_dir.mkdir(parents=True, exist_ok=True)

        # M√©tadonn√©es
        total_1m = len(btc_1m_trimmed) + len(eth_1m_trimmed)
        total_5m = len(btc_5m_trimmed) + len(eth_5m_trimmed)
        train_5m_seqs = len(X_btc_5m_train) + len(X_eth_5m_train)
        train_1m_seqs = len(X_btc_1m) + len(X_eth_1m)

        metadata = {
            'created_at': datetime.now().isoformat(),
            'timeframe': 'all',
            'filter_type': filter_type,
            'btc_candles': total_btc,
            'eth_candles': total_eth,
            'total_candles': total_1m + total_5m,
            'train_size': len(X_train),
            'val_size': len(X_val),
            'test_size': len(X_test),
            'train_composition': {
                '1min_sequences': train_1m_seqs,
                '5min_sequences': train_5m_seqs
            },
            'split_strategy': 'test=end, val=sampled',
            'val_test_source': '5min_only',
            'sequence_length': SEQUENCE_LENGTH,
            'num_indicators': NUM_INDICATORS,
            'indicator_params': {
                'rsi_period': RSI_PERIOD,
                'cci_period': CCI_PERIOD,
                'bol_period': BOL_PERIOD,
                'bol_num_std': BOL_NUM_STD,
                'macd_fast': MACD_FAST,
                'macd_slow': MACD_SLOW,
                'macd_signal': MACD_SIGNAL
            },
            'filter_params': {
                'decycler_cutoff': DECYCLER_CUTOFF,
                'kalman_process_var': KALMAN_PROCESS_VAR,
                'kalman_measure_var': KALMAN_MEASURE_VAR
            },
            'splits': {
                'train': TRAIN_SPLIT,
                'val': VAL_SPLIT,
                'test': TEST_SPLIT
            }
        }

        # Sauvegarder
        np.savez_compressed(
            output_path,
            X_train=X_train,
            Y_train=Y_train,
            X_val=X_val,
            Y_val=Y_val,
            X_test=X_test,
            Y_test=Y_test,
            metadata=json.dumps(metadata)
        )

        logger.info(f"\n‚úÖ Donn√©es sauvegard√©es: {output_path}")
        logger.info(f"   Taille: {Path(output_path).stat().st_size / 1024 / 1024:.1f} MB")

        # Sauvegarder m√©tadonn√©es
        metadata_path = str(output_path).replace('.npz', '_metadata.json')
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        logger.info(f"   M√©tadonn√©es: {metadata_path}")

        return output_path

    else:
        raise ValueError(f"Timeframe invalide: {timeframe}. Utilisez '1', '5', ou 'all'.")


def load_prepared_data(path: str = None) -> dict:
    """
    Charge les donn√©es pr√©par√©es depuis un fichier .npz.

    Args:
        path: Chemin vers le fichier .npz (d√©faut: PREPARED_DATA_FILE)

    Returns:
        Dictionnaire avec:
            'train': (X_train, Y_train)
            'val': (X_val, Y_val)
            'test': (X_test, Y_test)
            'metadata': dict avec les param√®tres utilis√©s
    """
    if path is None:
        # Chercher le fichier le plus r√©cent
        prepared_dir = Path(PREPARED_DATA_DIR)
        if prepared_dir.exists():
            npz_files = list(prepared_dir.glob('*.npz'))
            if npz_files:
                path = max(npz_files, key=lambda p: p.stat().st_mtime)
                logger.info(f"üìÇ Chargement du fichier le plus r√©cent: {path}")
            else:
                raise FileNotFoundError(f"Aucun fichier .npz trouv√© dans {prepared_dir}")
        else:
            raise FileNotFoundError(f"R√©pertoire {prepared_dir} non trouv√©")

    path = Path(path)
    if not path.exists():
        raise FileNotFoundError(f"Fichier non trouv√©: {path}")

    logger.info(f"üìÇ Chargement des donn√©es: {path}")

    data = np.load(path, allow_pickle=True)

    # Parser les m√©tadonn√©es
    metadata = json.loads(str(data['metadata']))

    result = {
        'train': (data['X_train'], data['Y_train']),
        'val': (data['X_val'], data['Y_val']),
        'test': (data['X_test'], data['Y_test']),
        'metadata': metadata
    }

    logger.info(f"  ‚úÖ Donn√©es charg√©es:")
    logger.info(f"     Train: {data['X_train'].shape}")
    logger.info(f"     Val:   {data['X_val'].shape}")
    logger.info(f"     Test:  {data['X_test'].shape}")

    # Support ancien format (timeframe) et nouveau format (feature_timeframe/label_timeframe)
    if 'timeframe' in metadata:
        tf = metadata['timeframe']
        tf_str = f"{tf}m" if tf != 'all' else "all (1m+5m train, 5m val/test)"
        logger.info(f"     Timeframe: {tf_str}")
    else:
        # Nouveau format avec labels 30min
        feat_tf = metadata.get('feature_timeframe', 'unknown')
        label_tf = metadata.get('label_timeframe', 'unknown')
        logger.info(f"     Features: {feat_tf}")
        logger.info(f"     Labels: {label_tf}")

    logger.info(f"     Filtre: {metadata['filter_type']}")

    return result


def list_prepared_datasets():
    """Liste tous les datasets pr√©par√©s disponibles."""
    prepared_dir = Path(PREPARED_DATA_DIR)

    if not prepared_dir.exists():
        print(f"‚ùå R√©pertoire {prepared_dir} non trouv√©")
        return

    npz_files = list(prepared_dir.glob('*.npz'))

    if not npz_files:
        print(f"‚ùå Aucun dataset pr√©par√© dans {prepared_dir}")
        return

    print(f"\nüìÅ Datasets disponibles ({len(npz_files)}):\n")

    for f in sorted(npz_files, key=lambda p: p.stat().st_mtime, reverse=True):
        # Charger m√©tadonn√©es
        metadata_path = str(f).replace('.npz', '_metadata.json')
        if Path(metadata_path).exists():
            with open(metadata_path) as mf:
                meta = json.load(mf)
            print(f"  üìä {f.name}")
            print(f"     Timeframe: {meta['timeframe']}m | Filtre: {meta['filter_type']}")
            print(f"     Train: {meta['train_size']:,} | Val: {meta['val_size']:,} | Test: {meta['test_size']:,}")
            print(f"     RSI={meta['indicator_params']['rsi_period']}, CCI={meta['indicator_params']['cci_period']}, MACD={meta['indicator_params']['macd_fast']}/{meta['indicator_params']['macd_slow']}")
            print(f"     Cr√©√©: {meta['created_at']}")
            print()
        else:
            print(f"  üìä {f.name} (pas de m√©tadonn√©es)")
            print()


def main():
    """Point d'entr√©e CLI."""
    parser = argparse.ArgumentParser(
        description="Pr√©pare et sauvegarde les datasets pour l'entra√Ænement",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
  python src/prepare_data.py --timeframe 5 --filter kalman
  python src/prepare_data.py --timeframe 1 --filter decycler
  python src/prepare_data.py --timeframe all --filter kalman  # 1min+5min combin√©s!
  python src/prepare_data.py --list
        """
    )

    parser.add_argument('--timeframe', '-t', type=str, default='5',
                        choices=['1', '5', 'all'],
                        help='Timeframe: 1, 5, ou all (1min+5min train, 5min val/test)')
    parser.add_argument('--filter', '-f', type=str, default=LABEL_FILTER_TYPE,
                        choices=['decycler', 'kalman'], help='Filtre pour les labels')
    parser.add_argument('--output', '-o', type=str, default=None,
                        help='Chemin de sortie (d√©faut: auto-g√©n√©r√©)')
    parser.add_argument('--btc-candles', type=int, default=None,
                        help='Nombre de bougies BTC (d√©faut: toutes)')
    parser.add_argument('--eth-candles', type=int, default=None,
                        help='Nombre de bougies ETH (d√©faut: toutes)')
    parser.add_argument('--list', '-l', action='store_true',
                        help='Liste les datasets pr√©par√©s disponibles')

    args = parser.parse_args()

    # Configurer logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(message)s'
    )

    if args.list:
        list_prepared_datasets()
        return

    # Pr√©parer et sauvegarder
    output_path = prepare_and_save(
        timeframe=args.timeframe,
        filter_type=args.filter,
        output_path=args.output,
        btc_candles=args.btc_candles,
        eth_candles=args.eth_candles
    )

    print(f"\nüéâ Termin√©! Dataset pr√™t: {output_path}")
    print(f"\nPour entra√Æner:")
    print(f"  python src/train.py --data {output_path}")


if __name__ == '__main__':
    main()
