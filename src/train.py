"""
Script d'entra√Ænement du mod√®le CNN-LSTM Multi-Output.

Pipeline complet:
    1. Charger les donn√©es (BTC + ETH)
    2. Pr√©parer les datasets (indicateurs + labels)
    3. Cr√©er DataLoaders PyTorch
    4. Entra√Æner le mod√®le avec early stopping
    5. Sauvegarder le meilleur mod√®le
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
import logging
from tqdm import tqdm
import json
from typing import Dict, Tuple
import sys
import argparse

logger = logging.getLogger(__name__)

# Import modules locaux
from constants import (
    BATCH_SIZE,
    LEARNING_RATE,
    NUM_EPOCHS,
    EARLY_STOPPING_PATIENCE,
    RANDOM_SEED,
    BEST_MODEL_PATH,
    MODELS_DIR,
    CHECKPOINTS_DIR
)
from data_utils import load_and_split_btc_eth
from indicators import prepare_datasets
from model import create_model, compute_metrics


class IndicatorDataset(Dataset):
    """
    Dataset PyTorch pour les s√©quences d'indicateurs.

    Args:
        X: Features (n_sequences, sequence_length, n_indicators)
        Y: Labels (n_sequences, n_outputs)
    """

    def __init__(self, X: np.ndarray, Y: np.ndarray):
        self.X = torch.FloatTensor(X)
        self.Y = torch.FloatTensor(Y)

    def __len__(self) -> int:
        return len(self.X)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        return self.X[idx], self.Y[idx]


class EarlyStopping:
    """
    Early stopping pour arr√™ter l'entra√Ænement si validation loss ne s'am√©liore pas.

    Args:
        patience: Nombre d'√©poques sans am√©lioration avant d'arr√™ter
        min_delta: Am√©lioration minimale pour consid√©rer comme am√©lioration
        mode: 'min' pour loss (lower is better), 'max' pour accuracy (higher is better)
    """

    def __init__(self, patience: int = 10, min_delta: float = 0.0, mode: str = 'min'):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best_score = None
        self.early_stop = False

    def __call__(self, score: float) -> bool:
        """
        V√©rifier si on doit arr√™ter.

        Args:
            score: M√©trique √† surveiller (loss ou accuracy)

        Returns:
            True si on doit arr√™ter, False sinon
        """
        if self.best_score is None:
            self.best_score = score
            return False

        # Mode 'min': lower is better (loss)
        if self.mode == 'min':
            improved = score < (self.best_score - self.min_delta)
        # Mode 'max': higher is better (accuracy)
        else:
            improved = score > (self.best_score + self.min_delta)

        if improved:
            self.best_score = score
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
                return True

        return False


def train_epoch(
    model: nn.Module,
    dataloader: DataLoader,
    loss_fn: nn.Module,
    optimizer: optim.Optimizer,
    device: str
) -> Dict[str, float]:
    """
    Entra√Æne le mod√®le sur une √©poque.

    Args:
        model: Mod√®le
        dataloader: DataLoader
        loss_fn: Loss function
        optimizer: Optimizer
        device: Device

    Returns:
        Dictionnaire avec loss et m√©triques
    """
    model.train()

    total_loss = 0.0
    all_predictions = []
    all_targets = []

    for X_batch, Y_batch in dataloader:
        # D√©placer sur device
        X_batch = X_batch.to(device)
        Y_batch = Y_batch.to(device)

        # Forward
        optimizer.zero_grad()
        outputs = model(X_batch)

        # Loss
        loss = loss_fn(outputs, Y_batch)

        # Backward
        loss.backward()
        optimizer.step()

        # Accumuler
        total_loss += loss.item() * X_batch.size(0)
        all_predictions.append(outputs.detach().cpu())
        all_targets.append(Y_batch.detach().cpu())

    # Moyennes
    avg_loss = total_loss / len(dataloader.dataset)

    # M√©triques
    all_predictions = torch.cat(all_predictions, dim=0)
    all_targets = torch.cat(all_targets, dim=0)
    metrics = compute_metrics(all_predictions, all_targets)
    metrics['loss'] = avg_loss

    return metrics


def validate_epoch(
    model: nn.Module,
    dataloader: DataLoader,
    loss_fn: nn.Module,
    device: str
) -> Dict[str, float]:
    """
    Valide le mod√®le sur une √©poque.

    Args:
        model: Mod√®le
        dataloader: DataLoader
        loss_fn: Loss function
        device: Device

    Returns:
        Dictionnaire avec loss et m√©triques
    """
    model.eval()

    total_loss = 0.0
    all_predictions = []
    all_targets = []

    with torch.no_grad():
        for X_batch, Y_batch in dataloader:
            # D√©placer sur device
            X_batch = X_batch.to(device)
            Y_batch = Y_batch.to(device)

            # Forward
            outputs = model(X_batch)

            # Loss
            loss = loss_fn(outputs, Y_batch)

            # Accumuler
            total_loss += loss.item() * X_batch.size(0)
            all_predictions.append(outputs.cpu())
            all_targets.append(Y_batch.cpu())

    # Moyennes
    avg_loss = total_loss / len(dataloader.dataset)

    # M√©triques
    all_predictions = torch.cat(all_predictions, dim=0)
    all_targets = torch.cat(all_targets, dim=0)
    metrics = compute_metrics(all_predictions, all_targets)
    metrics['loss'] = avg_loss

    return metrics


def train_model(
    train_loader: DataLoader,
    val_loader: DataLoader,
    model: nn.Module,
    loss_fn: nn.Module,
    optimizer: optim.Optimizer,
    device: str,
    num_epochs: int = NUM_EPOCHS,
    patience: int = EARLY_STOPPING_PATIENCE,
    save_path: str = BEST_MODEL_PATH
) -> Dict:
    """
    Boucle d'entra√Ænement compl√®te avec early stopping.

    Args:
        train_loader: DataLoader train
        val_loader: DataLoader validation
        model: Mod√®le
        loss_fn: Loss function
        optimizer: Optimizer
        device: Device
        num_epochs: Nombre max d'√©poques
        patience: Patience pour early stopping
        save_path: Chemin pour sauvegarder le meilleur mod√®le

    Returns:
        Historique de l'entra√Ænement
    """
    logger.info("="*80)
    logger.info("D√âBUT DE L'ENTRA√éNEMENT")
    logger.info("="*80)

    # Early stopping
    early_stopping = EarlyStopping(patience=patience, mode='min')

    # Historique
    history = {
        'train_loss': [],
        'train_accuracy': [],
        'val_loss': [],
        'val_accuracy': [],
        'best_epoch': 0,
        'best_val_loss': float('inf')
    }

    # Boucle d'entra√Ænement
    for epoch in range(num_epochs):
        logger.info(f"\n√âpoque {epoch+1}/{num_epochs}")

        # Train
        train_metrics = train_epoch(model, train_loader, loss_fn, optimizer, device)

        # Validation
        val_metrics = validate_epoch(model, val_loader, loss_fn, device)

        # Sauvegarder historique
        history['train_loss'].append(train_metrics['loss'])
        history['train_accuracy'].append(train_metrics['avg_accuracy'])
        history['val_loss'].append(val_metrics['loss'])
        history['val_accuracy'].append(val_metrics['avg_accuracy'])

        # Logger
        logger.info(f"  Train - Loss: {train_metrics['loss']:.4f}, "
                   f"Acc: {train_metrics['avg_accuracy']:.3f}, "
                   f"F1: {train_metrics['avg_f1']:.3f}")
        logger.info(f"  Val   - Loss: {val_metrics['loss']:.4f}, "
                   f"Acc: {val_metrics['avg_accuracy']:.3f}, "
                   f"F1: {val_metrics['avg_f1']:.3f}")

        # Sauvegarder meilleur mod√®le
        if val_metrics['loss'] < history['best_val_loss']:
            history['best_val_loss'] = val_metrics['loss']
            history['best_epoch'] = epoch + 1

            # Cr√©er dossier si n√©cessaire
            Path(save_path).parent.mkdir(parents=True, exist_ok=True)

            # Sauvegarder
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_metrics['loss'],
                'val_accuracy': val_metrics['avg_accuracy'],
            }, save_path)

            logger.info(f"  ‚úÖ Meilleur mod√®le sauvegard√© (val_loss: {val_metrics['loss']:.4f})")

        # Early stopping
        if early_stopping(val_metrics['loss']):
            logger.info(f"\n‚èπÔ∏è Early stopping √† l'√©poque {epoch+1}")
            break

    logger.info("="*80)
    logger.info("FIN DE L'ENTRA√éNEMENT")
    logger.info("="*80)
    logger.info(f"Meilleur mod√®le: √âpoque {history['best_epoch']}, "
               f"Val Loss: {history['best_val_loss']:.4f}")

    return history


def parse_args():
    """Parse les arguments de ligne de commande."""
    parser = argparse.ArgumentParser(
        description='Entra√Ænement du mod√®le CNN-LSTM Multi-Output',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # Hyperparam√®tres d'entra√Ænement
    parser.add_argument('--batch-size', type=int, default=BATCH_SIZE,
                        help='Taille des batches')
    parser.add_argument('--lr', '--learning-rate', type=float, default=LEARNING_RATE,
                        dest='learning_rate', help='Learning rate')
    parser.add_argument('--epochs', type=int, default=NUM_EPOCHS,
                        help='Nombre maximum d\'√©poques')
    parser.add_argument('--patience', type=int, default=EARLY_STOPPING_PATIENCE,
                        help='Patience pour early stopping')

    # Chemins
    parser.add_argument('--save-path', type=str, default=BEST_MODEL_PATH,
                        help='Chemin pour sauvegarder le meilleur mod√®le')

    # Autres
    parser.add_argument('--seed', type=int, default=RANDOM_SEED,
                        help='Random seed pour reproductibilit√©')
    parser.add_argument('--device', type=str, default='auto',
                        choices=['auto', 'cuda', 'cpu'],
                        help='Device √† utiliser (auto d√©tecte automatiquement)')

    return parser.parse_args()


def main():
    """Pipeline complet d'entra√Ænement."""

    # Parser arguments
    args = parse_args()

    # Configurer logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(levelname)s - %(message)s'
    )

    logger.info("="*80)
    logger.info("PIPELINE D'ENTRA√éNEMENT CNN-LSTM")
    logger.info("="*80)

    # Seed pour reproductibilit√©
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)

    # Device
    if args.device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    else:
        device = args.device
    logger.info(f"\nDevice: {device}")

    # Afficher hyperparam√®tres
    logger.info(f"\n‚öôÔ∏è Hyperparam√®tres:")
    logger.info(f"  Batch size: {args.batch_size}")
    logger.info(f"  Learning rate: {args.learning_rate}")
    logger.info(f"  Max epochs: {args.epochs}")
    logger.info(f"  Early stopping patience: {args.patience}")
    logger.info(f"  Random seed: {args.seed}")

    # =========================================================================
    # 1. CHARGER LES DONN√âES
    # =========================================================================
    logger.info("\n1. Chargement des donn√©es BTC + ETH...")
    train_df, val_df, test_df = load_and_split_btc_eth()

    # =========================================================================
    # 2. PR√âPARER LES DATASETS
    # =========================================================================
    logger.info("\n2. Pr√©paration des datasets (indicateurs + labels)...")
    datasets = prepare_datasets(train_df, val_df, test_df)

    X_train, Y_train = datasets['train']
    X_val, Y_val = datasets['val']
    X_test, Y_test = datasets['test']

    logger.info(f"\nüìä Datasets:")
    logger.info(f"  Train: X={X_train.shape}, Y={Y_train.shape}")
    logger.info(f"  Val:   X={X_val.shape}, Y={Y_val.shape}")
    logger.info(f"  Test:  X={X_test.shape}, Y={Y_test.shape}")

    # =========================================================================
    # 3. CR√âER DATALOADERS
    # =========================================================================
    logger.info("\n3. Cr√©ation des DataLoaders...")

    train_dataset = IndicatorDataset(X_train, Y_train)
    val_dataset = IndicatorDataset(X_val, Y_val)

    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=0,  # 0 pour √©viter probl√®mes multiprocessing
        pin_memory=True if device == 'cuda' else False
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True if device == 'cuda' else False
    )

    logger.info(f"  Train batches: {len(train_loader)}")
    logger.info(f"  Val batches: {len(val_loader)}")

    # =========================================================================
    # 4. CR√âER MOD√àLE
    # =========================================================================
    logger.info("\n4. Cr√©ation du mod√®le...")
    model, loss_fn = create_model(device=device)

    # Optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)

    # =========================================================================
    # 5. ENTRA√éNEMENT
    # =========================================================================
    logger.info(f"\n5. Entra√Ænement ({args.epochs} √©poques max)...")

    history = train_model(
        train_loader=train_loader,
        val_loader=val_loader,
        model=model,
        loss_fn=loss_fn,
        optimizer=optimizer,
        device=device,
        num_epochs=args.epochs,
        patience=args.patience,
        save_path=args.save_path
    )

    # =========================================================================
    # 6. SAUVEGARDER HISTORIQUE
    # =========================================================================
    logger.info("\n6. Sauvegarde de l'historique...")

    history_path = Path(MODELS_DIR) / 'training_history.json'
    history_path.parent.mkdir(parents=True, exist_ok=True)

    with open(history_path, 'w') as f:
        json.dump(history, f, indent=2)

    logger.info(f"  Historique sauvegard√©: {history_path}")

    # =========================================================================
    # R√âSUM√â FINAL
    # =========================================================================
    logger.info("\n" + "="*80)
    logger.info("‚úÖ ENTRA√éNEMENT TERMIN√â")
    logger.info("="*80)
    logger.info(f"\nMeilleur mod√®le:")
    logger.info(f"  √âpoque: {history['best_epoch']}")
    logger.info(f"  Val Loss: {history['best_val_loss']:.4f}")
    logger.info(f"  Sauvegard√©: {args.save_path}")

    logger.info(f"\nProchaines √©tapes:")
    logger.info(f"  - √âvaluer sur test set: python src/evaluate.py")
    logger.info(f"  - Visualiser historique: voir {history_path}")


if __name__ == '__main__':
    main()
